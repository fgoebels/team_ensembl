{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import (GRU, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, TimeDistributed, Flatten, Activation, \n",
    "                          Concatenate, Multiply, RepeatVector, Permute, Lambda, BatchNormalization, Add, CuDNNGRU, CuDNNLSTM, Conv1D, MaxPooling1D)\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from Attention import Attention\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import seq2seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "import hunspell\n",
    "import editdistance #Levenshtein distance\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    column_losses = []\n",
    "    for i in range(0, columns):\n",
    "        column_losses.append(log_loss(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(column_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 100000\n",
    "maxlen = 150\n",
    "\n",
    "train = pd.read_csv(\"./train_cleaned.csv\")\n",
    "test = pd.read_csv(\"./test_cleaned.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "word_index = tokenizer.word_index\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195895 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embed_size = 300\n",
    "\n",
    "f = open('./glove.840B.300d.txt') #Use this as our list of valid words\n",
    "valid_words = []\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(max_words, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 150, 300)     722400      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 150, 300)     722400      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 300)          450         cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 300)          450         cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 600)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          153856      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1542        batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 31,602,122\n",
      "Trainable params: 1,601,610\n",
      "Non-trainable params: 30,000,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    #x = Bidirectional(CuDNNLSTM(300, return_sequences=True))(x)\n",
    "    \n",
    "    \n",
    "    f = CuDNNLSTM(300, return_sequences=True)(x)\n",
    "    f= Attention(maxlen)(f)\n",
    "    b = CuDNNLSTM(300, return_sequences=True, go_backwards=True)(x)\n",
    "    b= Attention(maxlen)(b)\n",
    "\n",
    "    x = Concatenate()([f,b])\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    #x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "init_weights = model.get_weights()\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stratify KFold by creating unique labels for ground truth values, toss things with less than num_folds into an 'others' label\n",
    "num_folds = 10\n",
    "truth_labels = np.unique(y,axis=0)\n",
    "\n",
    "truth_counts = np.zeros(truth_labels.shape[0],dtype=int)\n",
    "for j, tlab in enumerate(truth_labels):\n",
    "    truth_counts[j] = (y == tlab).all(-1).sum()\n",
    "\n",
    "other_label = truth_labels.shape[0]+1 #create a new label out of the index range for 'others'\n",
    "truth_encoded = []\n",
    "for row in y:\n",
    "    for j, truth_row in enumerate(truth_labels):\n",
    "        index_test = np.array_equal(row, truth_row)\n",
    "        if index_test:\n",
    "            if truth_counts[j]>=num_folds:\n",
    "                truth_encoded.append(j)\n",
    "            else:\n",
    "                truth_encoded.append(other_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 0\n",
      "Train on 86253 samples, validate on 9598 samples\n",
      "Epoch 1/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9413Epoch 00001: val_loss improved from inf to 0.13983, saving model to fold_0.hdf5\n",
      "86253/86253 [==============================] - 64s 737us/step - loss: 0.2420 - acc: 0.9414 - val_loss: 0.1398 - val_acc: 0.9633\n",
      "Epoch 2/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9802Epoch 00002: val_loss improved from 0.13983 to 0.08115, saving model to fold_0.hdf5\n",
      "86253/86253 [==============================] - 63s 734us/step - loss: 0.0555 - acc: 0.9802 - val_loss: 0.0812 - val_acc: 0.9703\n",
      "Epoch 3/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9815Epoch 00003: val_loss improved from 0.08115 to 0.05148, saving model to fold_0.hdf5\n",
      "86253/86253 [==============================] - 63s 736us/step - loss: 0.0500 - acc: 0.9815 - val_loss: 0.0515 - val_acc: 0.9804\n",
      "Epoch 4/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9823Epoch 00004: val_loss improved from 0.05148 to 0.04759, saving model to fold_0.hdf5\n",
      "86253/86253 [==============================] - 63s 734us/step - loss: 0.0472 - acc: 0.9822 - val_loss: 0.0476 - val_acc: 0.9825\n",
      "Epoch 5/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9830Epoch 00005: val_loss improved from 0.04759 to 0.04272, saving model to fold_0.hdf5\n",
      "86253/86253 [==============================] - 63s 732us/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0427 - val_acc: 0.9837\n",
      "Epoch 6/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9836Epoch 00006: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 725us/step - loss: 0.0430 - acc: 0.9836 - val_loss: 0.0440 - val_acc: 0.9830\n",
      "Epoch 7/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9841Epoch 00007: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 731us/step - loss: 0.0412 - acc: 0.9840 - val_loss: 0.0472 - val_acc: 0.9821\n",
      "Epoch 8/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9844Epoch 00008: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 732us/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0433 - val_acc: 0.9834\n",
      "Epoch 9/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9851Epoch 00009: val_loss improved from 0.04272 to 0.04212, saving model to fold_0.hdf5\n",
      "86253/86253 [==============================] - 64s 737us/step - loss: 0.0386 - acc: 0.9851 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "Epoch 10/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9852Epoch 00010: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 734us/step - loss: 0.0374 - acc: 0.9852 - val_loss: 0.0444 - val_acc: 0.9833\n",
      "Epoch 11/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9856Epoch 00011: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 736us/step - loss: 0.0366 - acc: 0.9856 - val_loss: 0.0481 - val_acc: 0.9829\n",
      "Epoch 12/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9862Epoch 00012: val_loss did not improve\n",
      "86253/86253 [==============================] - 64s 738us/step - loss: 0.0353 - acc: 0.9862 - val_loss: 0.0434 - val_acc: 0.9833\n",
      "Epoch 13/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9864Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.0005000000237487257.\n",
      "86253/86253 [==============================] - 62s 722us/step - loss: 0.0341 - acc: 0.9864 - val_loss: 0.0436 - val_acc: 0.9836\n",
      "Epoch 14/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9876Epoch 00014: val_loss did not improve\n",
      "86253/86253 [==============================] - 62s 720us/step - loss: 0.0311 - acc: 0.9876 - val_loss: 0.0462 - val_acc: 0.9839\n",
      "Epoch 15/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9881Epoch 00015: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 730us/step - loss: 0.0300 - acc: 0.9881 - val_loss: 0.0452 - val_acc: 0.9833\n",
      "Epoch 16/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9885Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0002500000118743628.\n",
      "86253/86253 [==============================] - 62s 721us/step - loss: 0.0287 - acc: 0.9885 - val_loss: 0.0463 - val_acc: 0.9830\n",
      "Epoch 17/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9892Epoch 00017: val_loss did not improve\n",
      "86253/86253 [==============================] - 63s 730us/step - loss: 0.0271 - acc: 0.9892 - val_loss: 0.0468 - val_acc: 0.9832\n",
      "Epoch 18/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9894Epoch 00018: val_loss did not improve\n",
      "86253/86253 [==============================] - 62s 720us/step - loss: 0.0261 - acc: 0.9894 - val_loss: 0.0475 - val_acc: 0.9828\n",
      "Epoch 19/50\n",
      "86016/86253 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9896Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0001250000059371814.\n",
      "86253/86253 [==============================] - 63s 727us/step - loss: 0.0257 - acc: 0.9896 - val_loss: 0.0476 - val_acc: 0.9825\n",
      "9598/9598 [==============================] - 2s 224us/step\n",
      "Loss on fold 0 is 0.042120\n",
      "226998/226998 [==============================] - 51s 223us/step\n",
      "Training on fold 1\n",
      "Train on 86258 samples, validate on 9593 samples\n",
      "Epoch 1/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9387Epoch 00001: val_loss improved from inf to 0.12118, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 63s 734us/step - loss: 0.2448 - acc: 0.9388 - val_loss: 0.1212 - val_acc: 0.9637\n",
      "Epoch 2/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9804Epoch 00002: val_loss improved from 0.12118 to 0.09916, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 62s 724us/step - loss: 0.0550 - acc: 0.9804 - val_loss: 0.0992 - val_acc: 0.9672\n",
      "Epoch 3/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9812Epoch 00003: val_loss improved from 0.09916 to 0.04878, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 63s 729us/step - loss: 0.0517 - acc: 0.9811 - val_loss: 0.0488 - val_acc: 0.9820\n",
      "Epoch 4/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9821Epoch 00004: val_loss improved from 0.04878 to 0.04673, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 63s 728us/step - loss: 0.0478 - acc: 0.9821 - val_loss: 0.0467 - val_acc: 0.9826\n",
      "Epoch 5/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9827Epoch 00005: val_loss improved from 0.04673 to 0.04618, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 63s 732us/step - loss: 0.0451 - acc: 0.9827 - val_loss: 0.0462 - val_acc: 0.9827\n",
      "Epoch 6/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9833Epoch 00006: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 732us/step - loss: 0.0436 - acc: 0.9833 - val_loss: 0.0488 - val_acc: 0.9826\n",
      "Epoch 7/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00007: val_loss did not improve\n",
      "86258/86258 [==============================] - 64s 740us/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0463 - val_acc: 0.9831\n",
      "Epoch 8/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9844Epoch 00008: val_loss improved from 0.04618 to 0.04435, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 64s 745us/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.0443 - val_acc: 0.9839\n",
      "Epoch 9/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849Epoch 00009: val_loss did not improve\n",
      "86258/86258 [==============================] - 64s 742us/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0480 - val_acc: 0.9807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00010: val_loss did not improve\n",
      "86258/86258 [==============================] - 62s 722us/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0454 - val_acc: 0.9834\n",
      "Epoch 11/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9858Epoch 00011: val_loss did not improve\n",
      "86258/86258 [==============================] - 62s 724us/step - loss: 0.0360 - acc: 0.9858 - val_loss: 0.0463 - val_acc: 0.9826\n",
      "Epoch 12/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9861Epoch 00012: val_loss improved from 0.04435 to 0.04339, saving model to fold_1.hdf5\n",
      "86258/86258 [==============================] - 63s 730us/step - loss: 0.0353 - acc: 0.9861 - val_loss: 0.0434 - val_acc: 0.9841\n",
      "Epoch 13/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9865Epoch 00013: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 727us/step - loss: 0.0338 - acc: 0.9865 - val_loss: 0.0444 - val_acc: 0.9837\n",
      "Epoch 14/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9870Epoch 00014: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 730us/step - loss: 0.0325 - acc: 0.9870 - val_loss: 0.0453 - val_acc: 0.9837\n",
      "Epoch 15/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9875Epoch 00015: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 733us/step - loss: 0.0312 - acc: 0.9875 - val_loss: 0.0485 - val_acc: 0.9819\n",
      "Epoch 16/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9881Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0005000000237487257.\n",
      "86258/86258 [==============================] - 63s 731us/step - loss: 0.0298 - acc: 0.9881 - val_loss: 0.0458 - val_acc: 0.9834\n",
      "Epoch 17/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9892Epoch 00017: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 729us/step - loss: 0.0269 - acc: 0.9892 - val_loss: 0.0487 - val_acc: 0.9840\n",
      "Epoch 18/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9896Epoch 00018: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 728us/step - loss: 0.0257 - acc: 0.9896 - val_loss: 0.0493 - val_acc: 0.9835\n",
      "Epoch 19/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9901Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0002500000118743628.\n",
      "86258/86258 [==============================] - 63s 728us/step - loss: 0.0245 - acc: 0.9901 - val_loss: 0.0495 - val_acc: 0.9840\n",
      "Epoch 20/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9907Epoch 00020: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 727us/step - loss: 0.0229 - acc: 0.9907 - val_loss: 0.0533 - val_acc: 0.9838\n",
      "Epoch 21/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9912Epoch 00021: val_loss did not improve\n",
      "86258/86258 [==============================] - 63s 729us/step - loss: 0.0222 - acc: 0.9912 - val_loss: 0.0514 - val_acc: 0.9828\n",
      "Epoch 22/50\n",
      "86016/86258 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9912Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.0001250000059371814.\n",
      "86258/86258 [==============================] - 63s 730us/step - loss: 0.0216 - acc: 0.9912 - val_loss: 0.0517 - val_acc: 0.9830\n",
      "9593/9593 [==============================] - 2s 233us/step\n",
      "Loss on fold 1 is 0.043389\n",
      "226998/226998 [==============================] - 52s 228us/step\n",
      "Training on fold 2\n",
      "Train on 86261 samples, validate on 9590 samples\n",
      "Epoch 1/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9203Epoch 00001: val_loss improved from inf to 0.12105, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 64s 747us/step - loss: 0.2603 - acc: 0.9204 - val_loss: 0.1211 - val_acc: 0.9640\n",
      "Epoch 2/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9803Epoch 00002: val_loss improved from 0.12105 to 0.08473, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 63s 735us/step - loss: 0.0570 - acc: 0.9803 - val_loss: 0.0847 - val_acc: 0.9706\n",
      "Epoch 3/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9816Epoch 00003: val_loss improved from 0.08473 to 0.06009, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 63s 734us/step - loss: 0.0506 - acc: 0.9816 - val_loss: 0.0601 - val_acc: 0.9768\n",
      "Epoch 4/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9825Epoch 00004: val_loss improved from 0.06009 to 0.05168, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 63s 731us/step - loss: 0.0475 - acc: 0.9825 - val_loss: 0.0517 - val_acc: 0.9814\n",
      "Epoch 5/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9827Epoch 00005: val_loss improved from 0.05168 to 0.04457, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 63s 736us/step - loss: 0.0452 - acc: 0.9827 - val_loss: 0.0446 - val_acc: 0.9837\n",
      "Epoch 6/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9835Epoch 00006: val_loss improved from 0.04457 to 0.04205, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 63s 733us/step - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0421 - val_acc: 0.9840\n",
      "Epoch 7/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9839Epoch 00007: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 731us/step - loss: 0.0417 - acc: 0.9839 - val_loss: 0.0533 - val_acc: 0.9805\n",
      "Epoch 8/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9844Epoch 00008: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 732us/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0449 - val_acc: 0.9831\n",
      "Epoch 9/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9848Epoch 00009: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 731us/step - loss: 0.0393 - acc: 0.9848 - val_loss: 0.0426 - val_acc: 0.9835\n",
      "Epoch 10/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9851Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.0005000000237487257.\n",
      "86261/86261 [==============================] - 63s 733us/step - loss: 0.0383 - acc: 0.9851 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "Epoch 11/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9861Epoch 00011: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 730us/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0438 - val_acc: 0.9838\n",
      "Epoch 12/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9867Epoch 00012: val_loss improved from 0.04205 to 0.04169, saving model to fold_2.hdf5\n",
      "86261/86261 [==============================] - 63s 733us/step - loss: 0.0339 - acc: 0.9867 - val_loss: 0.0417 - val_acc: 0.9842\n",
      "Epoch 13/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9868Epoch 00013: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 731us/step - loss: 0.0333 - acc: 0.9868 - val_loss: 0.0430 - val_acc: 0.9846\n",
      "Epoch 14/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9873Epoch 00014: val_loss did not improve\n",
      "86261/86261 [==============================] - 64s 737us/step - loss: 0.0321 - acc: 0.9873 - val_loss: 0.0431 - val_acc: 0.9835\n",
      "Epoch 15/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9876Epoch 00015: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 733us/step - loss: 0.0310 - acc: 0.9876 - val_loss: 0.0442 - val_acc: 0.9843\n",
      "Epoch 16/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9879Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0002500000118743628.\n",
      "86261/86261 [==============================] - 62s 722us/step - loss: 0.0302 - acc: 0.9879 - val_loss: 0.0432 - val_acc: 0.9840\n",
      "Epoch 17/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9886Epoch 00017: val_loss did not improve\n",
      "86261/86261 [==============================] - 62s 720us/step - loss: 0.0284 - acc: 0.9886 - val_loss: 0.0463 - val_acc: 0.9832\n",
      "Epoch 18/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9890Epoch 00018: val_loss did not improve\n",
      "86261/86261 [==============================] - 62s 719us/step - loss: 0.0275 - acc: 0.9890 - val_loss: 0.0442 - val_acc: 0.9838\n",
      "Epoch 19/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9890Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0001250000059371814.\n",
      "86261/86261 [==============================] - 62s 720us/step - loss: 0.0271 - acc: 0.9890 - val_loss: 0.0455 - val_acc: 0.9840\n",
      "Epoch 20/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9895Epoch 00020: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 732us/step - loss: 0.0258 - acc: 0.9895 - val_loss: 0.0462 - val_acc: 0.9837\n",
      "Epoch 21/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9898Epoch 00021: val_loss did not improve\n",
      "86261/86261 [==============================] - 63s 728us/step - loss: 0.0252 - acc: 0.9897 - val_loss: 0.0470 - val_acc: 0.9835\n",
      "Epoch 22/50\n",
      "86016/86261 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9899Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.0001.\n",
      "86261/86261 [==============================] - 63s 732us/step - loss: 0.0250 - acc: 0.9899 - val_loss: 0.0477 - val_acc: 0.9839\n",
      "9590/9590 [==============================] - 2s 236us/step\n",
      "Loss on fold 2 is 0.042503\n",
      "226998/226998 [==============================] - 52s 230us/step\n",
      "Training on fold 3\n",
      "Train on 86263 samples, validate on 9588 samples\n",
      "Epoch 1/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9377Epoch 00001: val_loss improved from inf to 0.12644, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 64s 737us/step - loss: 0.2605 - acc: 0.9378 - val_loss: 0.1264 - val_acc: 0.9636\n",
      "Epoch 2/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9801Epoch 00002: val_loss improved from 0.12644 to 0.09025, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 64s 737us/step - loss: 0.0575 - acc: 0.9801 - val_loss: 0.0903 - val_acc: 0.9687\n",
      "Epoch 3/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9814Epoch 00003: val_loss improved from 0.09025 to 0.06057, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 63s 732us/step - loss: 0.0509 - acc: 0.9814 - val_loss: 0.0606 - val_acc: 0.9776\n",
      "Epoch 4/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9823Epoch 00004: val_loss improved from 0.06057 to 0.04967, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 63s 735us/step - loss: 0.0471 - acc: 0.9823 - val_loss: 0.0497 - val_acc: 0.9826\n",
      "Epoch 5/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9831Epoch 00005: val_loss improved from 0.04967 to 0.04297, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 63s 733us/step - loss: 0.0449 - acc: 0.9831 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "Epoch 6/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9836Epoch 00006: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 733us/step - loss: 0.0430 - acc: 0.9836 - val_loss: 0.0460 - val_acc: 0.9824\n",
      "Epoch 7/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9842Epoch 00007: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 732us/step - loss: 0.0414 - acc: 0.9842 - val_loss: 0.0446 - val_acc: 0.9836\n",
      "Epoch 8/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9840Epoch 00008: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 730us/step - loss: 0.0417 - acc: 0.9840 - val_loss: 0.0864 - val_acc: 0.9762\n",
      "Epoch 9/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9848Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.0005000000237487257.\n",
      "86263/86263 [==============================] - 63s 730us/step - loss: 0.0389 - acc: 0.9849 - val_loss: 0.0477 - val_acc: 0.9833\n",
      "Epoch 10/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9859Epoch 00010: val_loss improved from 0.04297 to 0.04219, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 64s 738us/step - loss: 0.0362 - acc: 0.9859 - val_loss: 0.0422 - val_acc: 0.9838\n",
      "Epoch 11/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9864Epoch 00011: val_loss improved from 0.04219 to 0.04169, saving model to fold_3.hdf5\n",
      "86263/86263 [==============================] - 63s 736us/step - loss: 0.0348 - acc: 0.9864 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "Epoch 12/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9867Epoch 00012: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 731us/step - loss: 0.0340 - acc: 0.9867 - val_loss: 0.0424 - val_acc: 0.9833\n",
      "Epoch 13/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9870Epoch 00013: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 730us/step - loss: 0.0331 - acc: 0.9870 - val_loss: 0.0426 - val_acc: 0.9836\n",
      "Epoch 14/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9874Epoch 00014: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 731us/step - loss: 0.0321 - acc: 0.9874 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 15/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9875Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.0002500000118743628.\n",
      "86263/86263 [==============================] - 63s 732us/step - loss: 0.0314 - acc: 0.9875 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 16/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9883Epoch 00016: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 730us/step - loss: 0.0296 - acc: 0.9883 - val_loss: 0.0435 - val_acc: 0.9837\n",
      "Epoch 17/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9885Epoch 00017: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 731us/step - loss: 0.0290 - acc: 0.9885 - val_loss: 0.0436 - val_acc: 0.9834\n",
      "Epoch 18/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9888Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.0001250000059371814.\n",
      "86263/86263 [==============================] - 63s 733us/step - loss: 0.0279 - acc: 0.9888 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 19/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9893Epoch 00019: val_loss did not improve\n",
      "86263/86263 [==============================] - 63s 733us/step - loss: 0.0269 - acc: 0.9893 - val_loss: 0.0452 - val_acc: 0.9833\n",
      "Epoch 20/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9894Epoch 00020: val_loss did not improve\n",
      "86263/86263 [==============================] - 64s 739us/step - loss: 0.0265 - acc: 0.9894 - val_loss: 0.0458 - val_acc: 0.9835\n",
      "Epoch 21/50\n",
      "86016/86263 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9894Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 0.0001.\n",
      "86263/86263 [==============================] - 63s 733us/step - loss: 0.0264 - acc: 0.9894 - val_loss: 0.0448 - val_acc: 0.9836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9588/9588 [==============================] - 2s 228us/step\n",
      "Loss on fold 3 is 0.041691\n",
      "226998/226998 [==============================] - 50s 222us/step\n",
      "Training on fold 4\n",
      "Train on 86264 samples, validate on 9587 samples\n",
      "Epoch 1/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9245Epoch 00001: val_loss improved from inf to 0.09736, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 734us/step - loss: 0.2808 - acc: 0.9247 - val_loss: 0.0974 - val_acc: 0.9680\n",
      "Epoch 2/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9789Epoch 00002: val_loss improved from 0.09736 to 0.07741, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 727us/step - loss: 0.0620 - acc: 0.9789 - val_loss: 0.0774 - val_acc: 0.9751\n",
      "Epoch 3/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9805Epoch 00003: val_loss improved from 0.07741 to 0.06550, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 726us/step - loss: 0.0534 - acc: 0.9805 - val_loss: 0.0655 - val_acc: 0.9797\n",
      "Epoch 4/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9815Epoch 00004: val_loss improved from 0.06550 to 0.04884, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 735us/step - loss: 0.0497 - acc: 0.9814 - val_loss: 0.0488 - val_acc: 0.9815\n",
      "Epoch 5/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9824Epoch 00005: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 728us/step - loss: 0.0466 - acc: 0.9823 - val_loss: 0.0491 - val_acc: 0.9827\n",
      "Epoch 6/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9830Epoch 00006: val_loss improved from 0.04884 to 0.04694, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 731us/step - loss: 0.0444 - acc: 0.9831 - val_loss: 0.0469 - val_acc: 0.9828\n",
      "Epoch 7/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9836Epoch 00007: val_loss improved from 0.04694 to 0.04553, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 735us/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0455 - val_acc: 0.9834\n",
      "Epoch 8/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9841Epoch 00008: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 734us/step - loss: 0.0411 - acc: 0.9840 - val_loss: 0.0465 - val_acc: 0.9825\n",
      "Epoch 9/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9843Epoch 00009: val_loss improved from 0.04553 to 0.04280, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 64s 737us/step - loss: 0.0402 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9838\n",
      "Epoch 10/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849Epoch 00010: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 733us/step - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "Epoch 11/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9855Epoch 00011: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 733us/step - loss: 0.0373 - acc: 0.9855 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "Epoch 12/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9858Epoch 00012: val_loss improved from 0.04280 to 0.04269, saving model to fold_4.hdf5\n",
      "86264/86264 [==============================] - 63s 728us/step - loss: 0.0362 - acc: 0.9858 - val_loss: 0.0427 - val_acc: 0.9840\n",
      "Epoch 13/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9865Epoch 00013: val_loss did not improve\n",
      "86264/86264 [==============================] - 64s 740us/step - loss: 0.0343 - acc: 0.9864 - val_loss: 0.0452 - val_acc: 0.9834\n",
      "Epoch 14/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9867Epoch 00014: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 734us/step - loss: 0.0329 - acc: 0.9867 - val_loss: 0.0514 - val_acc: 0.9830\n",
      "Epoch 15/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9868Epoch 00015: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 735us/step - loss: 0.0337 - acc: 0.9868 - val_loss: 0.0462 - val_acc: 0.9834\n",
      "Epoch 16/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9873Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0005000000237487257.\n",
      "86264/86264 [==============================] - 64s 739us/step - loss: 0.0314 - acc: 0.9873 - val_loss: 0.0467 - val_acc: 0.9831\n",
      "Epoch 17/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9888Epoch 00017: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 732us/step - loss: 0.0279 - acc: 0.9888 - val_loss: 0.0459 - val_acc: 0.9839\n",
      "Epoch 18/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9894Epoch 00018: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 732us/step - loss: 0.0263 - acc: 0.9894 - val_loss: 0.0502 - val_acc: 0.9837\n",
      "Epoch 19/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9897Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0002500000118743628.\n",
      "86264/86264 [==============================] - 63s 734us/step - loss: 0.0255 - acc: 0.9897 - val_loss: 0.0503 - val_acc: 0.9838\n",
      "Epoch 20/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9905Epoch 00020: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 732us/step - loss: 0.0236 - acc: 0.9905 - val_loss: 0.0496 - val_acc: 0.9835\n",
      "Epoch 21/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9908Epoch 00021: val_loss did not improve\n",
      "86264/86264 [==============================] - 63s 733us/step - loss: 0.0229 - acc: 0.9908 - val_loss: 0.0507 - val_acc: 0.9830\n",
      "Epoch 22/50\n",
      "86016/86264 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9911Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.0001250000059371814.\n",
      "86264/86264 [==============================] - 63s 734us/step - loss: 0.0222 - acc: 0.9911 - val_loss: 0.0528 - val_acc: 0.9829\n",
      "9587/9587 [==============================] - 2s 234us/step\n",
      "Loss on fold 4 is 0.042693\n",
      "226998/226998 [==============================] - 52s 229us/step\n",
      "Training on fold 5\n",
      "Train on 86268 samples, validate on 9583 samples\n",
      "Epoch 1/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9280Epoch 00001: val_loss improved from inf to 0.17800, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 735us/step - loss: 0.2660 - acc: 0.9281 - val_loss: 0.1780 - val_acc: 0.9640\n",
      "Epoch 2/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9798Epoch 00002: val_loss improved from 0.17800 to 0.10460, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 732us/step - loss: 0.0596 - acc: 0.9798 - val_loss: 0.1046 - val_acc: 0.9697\n",
      "Epoch 3/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9812Epoch 00003: val_loss improved from 0.10460 to 0.07025, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0509 - acc: 0.9812 - val_loss: 0.0703 - val_acc: 0.9781\n",
      "Epoch 4/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9820Epoch 00004: val_loss improved from 0.07025 to 0.04956, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0477 - acc: 0.9821 - val_loss: 0.0496 - val_acc: 0.9809\n",
      "Epoch 5/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9828Epoch 00005: val_loss improved from 0.04956 to 0.04534, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 733us/step - loss: 0.0454 - acc: 0.9827 - val_loss: 0.0453 - val_acc: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9835Epoch 00006: val_loss improved from 0.04534 to 0.04393, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 727us/step - loss: 0.0432 - acc: 0.9835 - val_loss: 0.0439 - val_acc: 0.9831\n",
      "Epoch 7/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9839Epoch 00007: val_loss did not improve\n",
      "86268/86268 [==============================] - 62s 724us/step - loss: 0.0416 - acc: 0.9839 - val_loss: 0.0453 - val_acc: 0.9827\n",
      "Epoch 8/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9845Epoch 00008: val_loss improved from 0.04393 to 0.04241, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 729us/step - loss: 0.0402 - acc: 0.9845 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 9/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00009: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 726us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0442 - val_acc: 0.9842\n",
      "Epoch 10/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00010: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 726us/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0426 - val_acc: 0.9841\n",
      "Epoch 11/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9857Epoch 00011: val_loss improved from 0.04241 to 0.04145, saving model to fold_5.hdf5\n",
      "86268/86268 [==============================] - 63s 730us/step - loss: 0.0367 - acc: 0.9857 - val_loss: 0.0415 - val_acc: 0.9839\n",
      "Epoch 12/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00012: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 730us/step - loss: 0.0350 - acc: 0.9861 - val_loss: 0.0433 - val_acc: 0.9835\n",
      "Epoch 13/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9860Epoch 00013: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0357 - acc: 0.9860 - val_loss: 0.0448 - val_acc: 0.9827\n",
      "Epoch 14/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9871Epoch 00014: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0327 - acc: 0.9871 - val_loss: 0.0479 - val_acc: 0.9833\n",
      "Epoch 15/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9868Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.0005000000237487257.\n",
      "86268/86268 [==============================] - 64s 736us/step - loss: 0.0373 - acc: 0.9868 - val_loss: 0.0439 - val_acc: 0.9835\n",
      "Epoch 16/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9884Epoch 00016: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0289 - acc: 0.9884 - val_loss: 0.0461 - val_acc: 0.9827\n",
      "Epoch 17/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9891Epoch 00017: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 730us/step - loss: 0.0275 - acc: 0.9891 - val_loss: 0.0443 - val_acc: 0.9839\n",
      "Epoch 18/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9894Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.0002500000118743628.\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0264 - acc: 0.9894 - val_loss: 0.0496 - val_acc: 0.9816\n",
      "Epoch 19/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9902Epoch 00019: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 733us/step - loss: 0.0245 - acc: 0.9902 - val_loss: 0.0465 - val_acc: 0.9839\n",
      "Epoch 20/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9906Epoch 00020: val_loss did not improve\n",
      "86268/86268 [==============================] - 63s 731us/step - loss: 0.0235 - acc: 0.9906 - val_loss: 0.0469 - val_acc: 0.9837\n",
      "Epoch 21/50\n",
      "86016/86268 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9909Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 0.0001250000059371814.\n",
      "86268/86268 [==============================] - 63s 726us/step - loss: 0.0228 - acc: 0.9909 - val_loss: 0.0496 - val_acc: 0.9838\n",
      "9583/9583 [==============================] - 2s 238us/step\n",
      "Loss on fold 5 is 0.041454\n",
      "226998/226998 [==============================] - 52s 227us/step\n",
      "Training on fold 6\n",
      "Train on 86269 samples, validate on 9582 samples\n",
      "Epoch 1/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9420Epoch 00001: val_loss improved from inf to 0.12600, saving model to fold_6.hdf5\n",
      "86269/86269 [==============================] - 64s 738us/step - loss: 0.2386 - acc: 0.9421 - val_loss: 0.1260 - val_acc: 0.9640\n",
      "Epoch 2/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9806Epoch 00002: val_loss improved from 0.12600 to 0.09835, saving model to fold_6.hdf5\n",
      "86269/86269 [==============================] - 63s 730us/step - loss: 0.0549 - acc: 0.9806 - val_loss: 0.0983 - val_acc: 0.9666\n",
      "Epoch 3/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9818Epoch 00003: val_loss improved from 0.09835 to 0.07263, saving model to fold_6.hdf5\n",
      "86269/86269 [==============================] - 63s 730us/step - loss: 0.0493 - acc: 0.9819 - val_loss: 0.0726 - val_acc: 0.9755\n",
      "Epoch 4/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9824Epoch 00004: val_loss improved from 0.07263 to 0.04870, saving model to fold_6.hdf5\n",
      "86269/86269 [==============================] - 63s 731us/step - loss: 0.0469 - acc: 0.9824 - val_loss: 0.0487 - val_acc: 0.9828\n",
      "Epoch 5/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9831Epoch 00005: val_loss improved from 0.04870 to 0.04397, saving model to fold_6.hdf5\n",
      "86269/86269 [==============================] - 63s 734us/step - loss: 0.0446 - acc: 0.9831 - val_loss: 0.0440 - val_acc: 0.9833\n",
      "Epoch 6/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9835Epoch 00006: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 733us/step - loss: 0.0429 - acc: 0.9835 - val_loss: 0.0446 - val_acc: 0.9834\n",
      "Epoch 7/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9840Epoch 00007: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 734us/step - loss: 0.0414 - acc: 0.9841 - val_loss: 0.0469 - val_acc: 0.9833\n",
      "Epoch 8/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9843Epoch 00008: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 735us/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0452 - val_acc: 0.9835\n",
      "Epoch 9/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9850Epoch 00009: val_loss improved from 0.04397 to 0.04282, saving model to fold_6.hdf5\n",
      "86269/86269 [==============================] - 63s 735us/step - loss: 0.0388 - acc: 0.9849 - val_loss: 0.0428 - val_acc: 0.9841\n",
      "Epoch 10/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9854Epoch 00010: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 734us/step - loss: 0.0374 - acc: 0.9854 - val_loss: 0.0447 - val_acc: 0.9846\n",
      "Epoch 11/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9857Epoch 00011: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 734us/step - loss: 0.0362 - acc: 0.9857 - val_loss: 0.0484 - val_acc: 0.9832\n",
      "Epoch 12/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9860Epoch 00012: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 735us/step - loss: 0.0358 - acc: 0.9860 - val_loss: 0.0436 - val_acc: 0.9833\n",
      "Epoch 13/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9865Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.0005000000237487257.\n",
      "86269/86269 [==============================] - 63s 729us/step - loss: 0.0340 - acc: 0.9865 - val_loss: 0.0448 - val_acc: 0.9832\n",
      "Epoch 14/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9878Epoch 00014: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 726us/step - loss: 0.0307 - acc: 0.9878 - val_loss: 0.0440 - val_acc: 0.9837\n",
      "Epoch 15/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9882Epoch 00015: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 730us/step - loss: 0.0293 - acc: 0.9882 - val_loss: 0.0457 - val_acc: 0.9837\n",
      "Epoch 16/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9887Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0002500000118743628.\n",
      "86269/86269 [==============================] - 63s 727us/step - loss: 0.0284 - acc: 0.9887 - val_loss: 0.0452 - val_acc: 0.9835\n",
      "Epoch 17/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9894Epoch 00017: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 728us/step - loss: 0.0265 - acc: 0.9894 - val_loss: 0.0453 - val_acc: 0.9839\n",
      "Epoch 18/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9899Epoch 00018: val_loss did not improve\n",
      "86269/86269 [==============================] - 63s 730us/step - loss: 0.0255 - acc: 0.9899 - val_loss: 0.0476 - val_acc: 0.9840\n",
      "Epoch 19/50\n",
      "86016/86269 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9899Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0001250000059371814.\n",
      "86269/86269 [==============================] - 63s 730us/step - loss: 0.0251 - acc: 0.9899 - val_loss: 0.0479 - val_acc: 0.9830\n",
      "9582/9582 [==============================] - 2s 234us/step\n",
      "Loss on fold 6 is 0.042893\n",
      "226998/226998 [==============================] - 52s 228us/step\n",
      "Training on fold 7\n",
      "Train on 86271 samples, validate on 9580 samples\n",
      "Epoch 1/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9207Epoch 00001: val_loss improved from inf to 0.10742, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 63s 734us/step - loss: 0.2945 - acc: 0.9209 - val_loss: 0.1074 - val_acc: 0.9661\n",
      "Epoch 2/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9799Epoch 00002: val_loss improved from 0.10742 to 0.09614, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 63s 731us/step - loss: 0.0592 - acc: 0.9799 - val_loss: 0.0961 - val_acc: 0.9700\n",
      "Epoch 3/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9812Epoch 00003: val_loss improved from 0.09614 to 0.05994, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 63s 730us/step - loss: 0.0515 - acc: 0.9812 - val_loss: 0.0599 - val_acc: 0.9790\n",
      "Epoch 4/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9820Epoch 00004: val_loss improved from 0.05994 to 0.04347, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 63s 729us/step - loss: 0.0476 - acc: 0.9820 - val_loss: 0.0435 - val_acc: 0.9840\n",
      "Epoch 5/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9828Epoch 00005: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 729us/step - loss: 0.0453 - acc: 0.9828 - val_loss: 0.0461 - val_acc: 0.9826\n",
      "Epoch 6/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9835Epoch 00006: val_loss improved from 0.04347 to 0.04143, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 63s 734us/step - loss: 0.0429 - acc: 0.9835 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 7/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9839Epoch 00007: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 728us/step - loss: 0.0418 - acc: 0.9839 - val_loss: 0.0467 - val_acc: 0.9825\n",
      "Epoch 8/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9842Epoch 00008: val_loss improved from 0.04143 to 0.04072, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 63s 734us/step - loss: 0.0408 - acc: 0.9842 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 9/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00009: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 735us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "Epoch 10/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00010: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 733us/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0414 - val_acc: 0.9842\n",
      "Epoch 11/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855Epoch 00011: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 732us/step - loss: 0.0367 - acc: 0.9855 - val_loss: 0.0422 - val_acc: 0.9847\n",
      "Epoch 12/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9861Epoch 00012: val_loss improved from 0.04072 to 0.04052, saving model to fold_7.hdf5\n",
      "86271/86271 [==============================] - 64s 736us/step - loss: 0.0352 - acc: 0.9861 - val_loss: 0.0405 - val_acc: 0.9846\n",
      "Epoch 13/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9865Epoch 00013: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 732us/step - loss: 0.0339 - acc: 0.9865 - val_loss: 0.0424 - val_acc: 0.9842\n",
      "Epoch 14/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9870Epoch 00014: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 731us/step - loss: 0.0326 - acc: 0.9870 - val_loss: 0.0436 - val_acc: 0.9838\n",
      "Epoch 15/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9875Epoch 00015: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 729us/step - loss: 0.0310 - acc: 0.9875 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "Epoch 16/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9882Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0005000000237487257.\n",
      "86271/86271 [==============================] - 63s 730us/step - loss: 0.0297 - acc: 0.9882 - val_loss: 0.0500 - val_acc: 0.9810\n",
      "Epoch 17/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9891Epoch 00017: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 730us/step - loss: 0.0269 - acc: 0.9891 - val_loss: 0.0441 - val_acc: 0.9840\n",
      "Epoch 18/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9897Epoch 00018: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 731us/step - loss: 0.0253 - acc: 0.9897 - val_loss: 0.0434 - val_acc: 0.9842\n",
      "Epoch 19/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9901Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0002500000118743628.\n",
      "86271/86271 [==============================] - 63s 733us/step - loss: 0.0244 - acc: 0.9901 - val_loss: 0.0463 - val_acc: 0.9841\n",
      "Epoch 20/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9909Epoch 00020: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 732us/step - loss: 0.0225 - acc: 0.9909 - val_loss: 0.0465 - val_acc: 0.9842\n",
      "Epoch 21/50\n",
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9912Epoch 00021: val_loss did not improve\n",
      "86271/86271 [==============================] - 63s 735us/step - loss: 0.0216 - acc: 0.9912 - val_loss: 0.0476 - val_acc: 0.9836\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86016/86271 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9914Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.0001250000059371814.\n",
      "86271/86271 [==============================] - 63s 729us/step - loss: 0.0212 - acc: 0.9914 - val_loss: 0.0490 - val_acc: 0.9831\n",
      "9580/9580 [==============================] - 2s 229us/step\n",
      "Loss on fold 7 is 0.040522\n",
      "226998/226998 [==============================] - 51s 223us/step\n",
      "Training on fold 8\n",
      "Train on 86275 samples, validate on 9576 samples\n",
      "Epoch 1/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9371Epoch 00001: val_loss improved from inf to 0.11617, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 732us/step - loss: 0.2523 - acc: 0.9373 - val_loss: 0.1162 - val_acc: 0.9642\n",
      "Epoch 2/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9803Epoch 00002: val_loss improved from 0.11617 to 0.10184, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 731us/step - loss: 0.0565 - acc: 0.9803 - val_loss: 0.1018 - val_acc: 0.9661\n",
      "Epoch 3/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9817Epoch 00003: val_loss improved from 0.10184 to 0.07476, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 731us/step - loss: 0.0501 - acc: 0.9817 - val_loss: 0.0748 - val_acc: 0.9739\n",
      "Epoch 4/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9823Epoch 00004: val_loss improved from 0.07476 to 0.05013, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 731us/step - loss: 0.0466 - acc: 0.9824 - val_loss: 0.0501 - val_acc: 0.9815\n",
      "Epoch 5/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9831Epoch 00005: val_loss improved from 0.05013 to 0.04655, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 732us/step - loss: 0.0448 - acc: 0.9831 - val_loss: 0.0466 - val_acc: 0.9828\n",
      "Epoch 6/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9837Epoch 00006: val_loss improved from 0.04655 to 0.04562, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 734us/step - loss: 0.0433 - acc: 0.9837 - val_loss: 0.0456 - val_acc: 0.9823\n",
      "Epoch 7/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9838Epoch 00007: val_loss did not improve\n",
      "86275/86275 [==============================] - 63s 726us/step - loss: 0.0419 - acc: 0.9838 - val_loss: 0.0462 - val_acc: 0.9834\n",
      "Epoch 8/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9843Epoch 00008: val_loss improved from 0.04562 to 0.04468, saving model to fold_8.hdf5\n",
      "86275/86275 [==============================] - 63s 731us/step - loss: 0.0401 - acc: 0.9843 - val_loss: 0.0447 - val_acc: 0.9833\n",
      "Epoch 9/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9849Epoch 00009: val_loss did not improve\n",
      "86275/86275 [==============================] - 63s 729us/step - loss: 0.0389 - acc: 0.9849 - val_loss: 0.0467 - val_acc: 0.9832\n",
      "Epoch 10/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9853Epoch 00010: val_loss did not improve\n",
      "86275/86275 [==============================] - 63s 734us/step - loss: 0.0377 - acc: 0.9853 - val_loss: 0.0471 - val_acc: 0.9829\n",
      "Epoch 11/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9857Epoch 00011: val_loss did not improve\n",
      "86275/86275 [==============================] - 63s 733us/step - loss: 0.0366 - acc: 0.9857 - val_loss: 0.0487 - val_acc: 0.9828\n",
      "Epoch 12/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.0005000000237487257.\n",
      "86275/86275 [==============================] - 64s 736us/step - loss: 0.0351 - acc: 0.9862 - val_loss: 0.0456 - val_acc: 0.9833\n",
      "Epoch 13/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9872Epoch 00013: val_loss did not improve\n",
      "86275/86275 [==============================] - 63s 732us/step - loss: 0.0324 - acc: 0.9872 - val_loss: 0.0457 - val_acc: 0.9833\n",
      "Epoch 14/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9876Epoch 00014: val_loss did not improve\n",
      "86275/86275 [==============================] - 64s 737us/step - loss: 0.0313 - acc: 0.9876 - val_loss: 0.0467 - val_acc: 0.9826\n",
      "Epoch 15/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9881Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.0002500000118743628.\n",
      "86275/86275 [==============================] - 63s 735us/step - loss: 0.0302 - acc: 0.9880 - val_loss: 0.0483 - val_acc: 0.9826\n",
      "Epoch 16/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9886Epoch 00016: val_loss did not improve\n",
      "86275/86275 [==============================] - 64s 737us/step - loss: 0.0283 - acc: 0.9886 - val_loss: 0.0470 - val_acc: 0.9836\n",
      "Epoch 17/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9889Epoch 00017: val_loss did not improve\n",
      "86275/86275 [==============================] - 63s 733us/step - loss: 0.0277 - acc: 0.9889 - val_loss: 0.0487 - val_acc: 0.9830\n",
      "Epoch 18/50\n",
      "86016/86275 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9891Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.0001250000059371814.\n",
      "86275/86275 [==============================] - 63s 736us/step - loss: 0.0272 - acc: 0.9891 - val_loss: 0.0500 - val_acc: 0.9825\n",
      "9576/9576 [==============================] - 2s 234us/step\n",
      "Loss on fold 8 is 0.044684\n",
      "226998/226998 [==============================] - 52s 229us/step\n",
      "Training on fold 9\n",
      "Train on 86277 samples, validate on 9574 samples\n",
      "Epoch 1/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9398Epoch 00001: val_loss improved from inf to 0.11793, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 64s 739us/step - loss: 0.2418 - acc: 0.9399 - val_loss: 0.1179 - val_acc: 0.9643\n",
      "Epoch 2/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9800Epoch 00002: val_loss improved from 0.11793 to 0.09517, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 64s 737us/step - loss: 0.0562 - acc: 0.9800 - val_loss: 0.0952 - val_acc: 0.9686\n",
      "Epoch 3/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9817Epoch 00003: val_loss improved from 0.09517 to 0.05125, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 63s 734us/step - loss: 0.0499 - acc: 0.9817 - val_loss: 0.0513 - val_acc: 0.9807\n",
      "Epoch 4/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9824Epoch 00004: val_loss improved from 0.05125 to 0.04765, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 63s 733us/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0477 - val_acc: 0.9818\n",
      "Epoch 5/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9829Epoch 00005: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 731us/step - loss: 0.0449 - acc: 0.9829 - val_loss: 0.0490 - val_acc: 0.9812\n",
      "Epoch 6/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9837Epoch 00006: val_loss improved from 0.04765 to 0.04551, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 64s 737us/step - loss: 0.0427 - acc: 0.9837 - val_loss: 0.0455 - val_acc: 0.9831\n",
      "Epoch 7/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9840Epoch 00007: val_loss improved from 0.04551 to 0.04549, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 64s 739us/step - loss: 0.0422 - acc: 0.9840 - val_loss: 0.0455 - val_acc: 0.9827\n",
      "Epoch 8/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9844Epoch 00008: val_loss improved from 0.04549 to 0.04517, saving model to fold_9.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86277/86277 [==============================] - 64s 737us/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.0452 - val_acc: 0.9831\n",
      "Epoch 9/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9849Epoch 00009: val_loss did not improve\n",
      "86277/86277 [==============================] - 62s 722us/step - loss: 0.0390 - acc: 0.9849 - val_loss: 0.0612 - val_acc: 0.9826\n",
      "Epoch 10/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00010: val_loss improved from 0.04517 to 0.04489, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 63s 733us/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0449 - val_acc: 0.9832\n",
      "Epoch 11/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9858Epoch 00011: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 729us/step - loss: 0.0363 - acc: 0.9858 - val_loss: 0.0457 - val_acc: 0.9838\n",
      "Epoch 12/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9860Epoch 00012: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 727us/step - loss: 0.0351 - acc: 0.9860 - val_loss: 0.0458 - val_acc: 0.9831\n",
      "Epoch 13/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9865Epoch 00013: val_loss improved from 0.04489 to 0.04422, saving model to fold_9.hdf5\n",
      "86277/86277 [==============================] - 63s 731us/step - loss: 0.0338 - acc: 0.9865 - val_loss: 0.0442 - val_acc: 0.9827\n",
      "Epoch 14/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9871Epoch 00014: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 726us/step - loss: 0.0324 - acc: 0.9871 - val_loss: 0.0561 - val_acc: 0.9814\n",
      "Epoch 15/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9877Epoch 00015: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 727us/step - loss: 0.0310 - acc: 0.9877 - val_loss: 0.0467 - val_acc: 0.9833\n",
      "Epoch 16/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9879Epoch 00016: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 730us/step - loss: 0.0299 - acc: 0.9880 - val_loss: 0.0517 - val_acc: 0.9825\n",
      "Epoch 17/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9884Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 0.0005000000237487257.\n",
      "86277/86277 [==============================] - 63s 734us/step - loss: 0.0287 - acc: 0.9884 - val_loss: 0.0466 - val_acc: 0.9826\n",
      "Epoch 18/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9900Epoch 00018: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 730us/step - loss: 0.0253 - acc: 0.9900 - val_loss: 0.0501 - val_acc: 0.9825\n",
      "Epoch 19/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9905Epoch 00019: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 731us/step - loss: 0.0237 - acc: 0.9905 - val_loss: 0.0524 - val_acc: 0.9827\n",
      "Epoch 20/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9907Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.0002500000118743628.\n",
      "86277/86277 [==============================] - 63s 732us/step - loss: 0.0231 - acc: 0.9907 - val_loss: 0.0508 - val_acc: 0.9827\n",
      "Epoch 21/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9915Epoch 00021: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 728us/step - loss: 0.0213 - acc: 0.9915 - val_loss: 0.0543 - val_acc: 0.9823\n",
      "Epoch 22/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9919Epoch 00022: val_loss did not improve\n",
      "86277/86277 [==============================] - 63s 730us/step - loss: 0.0201 - acc: 0.9919 - val_loss: 0.0548 - val_acc: 0.9826\n",
      "Epoch 23/50\n",
      "86016/86277 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9921Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 0.0001250000059371814.\n",
      "86277/86277 [==============================] - 63s 731us/step - loss: 0.0197 - acc: 0.9921 - val_loss: 0.0616 - val_acc: 0.9807\n",
      "9574/9574 [==============================] - 2s 240us/step\n",
      "Loss on fold 9 is 0.044367\n",
      "226998/226998 [==============================] - 52s 228us/step\n",
      "CV loss is 0.042631\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 50\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=42, shuffle=True)\n",
    "y_test = np.zeros((len(X_te),6))\n",
    "valpred = np.zeros((len(X_t),6))\n",
    "for j, (train_index, val_index) in enumerate(kf.split(X_t, truth_encoded)):\n",
    "    print(\"Training on fold {:d}\".format(j))\n",
    "    #model.set_weights(init_weights)\n",
    "    K.clear_session()\n",
    "    model = get_model()\n",
    "    early = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',  patience=3, verbose=1, factor=0.5, min_lr=0.0001)\n",
    "    checkpoint = ModelCheckpoint(\"fold_\" + str(j) + \".hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    history = model.fit(X_t[train_index], y[train_index], batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(X_t[val_index], y[val_index]), callbacks=[checkpoint, early, learning_rate_reduction])\n",
    "    \n",
    "    model.load_weights(\"fold_\" + str(j) + \".hdf5\")\n",
    "    \n",
    "    valpred[val_index,:]=model.predict(X_t[val_index], verbose=1, batch_size=512)\n",
    "    logloss = metric(y[val_index],valpred[val_index,:])\n",
    "    print(\"Loss on fold {:d} is {:f}\".format(j,logloss))\n",
    "    y_test += model.predict(X_te, verbose=1, batch_size=512)\n",
    "    \n",
    "y_test = y_test/num_folds\n",
    "logloss = metric(y,valpred)\n",
    "print(\"CV loss is {:f}\".format(logloss))\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "temp = pd.read_csv(\"./train.csv\")\n",
    "temp[list_classes] = valpred\n",
    "temp.to_csv(\"validation_predictions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
