{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import (GRU, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, TimeDistributed, Flatten, Activation, \n",
    "                          Concatenate, Multiply, RepeatVector, Permute, Lambda, BatchNormalization, Add, CuDNNGRU, CuDNNLSTM, Conv1D, MaxPooling1D)\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import seq2seq\n",
    "from recurrentshop import LSTMCell, RecurrentSequential\n",
    "from seq2seq.cells import LSTMDecoderCell, AttentionDecoderCell\n",
    "\n",
    "from tqdm import tqdm\n",
    "import hunspell\n",
    "import editdistance #Levenshtein distance\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer=EnglishStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    column_losses = []\n",
    "    for i in range(0, columns):\n",
    "        column_losses.append(log_loss(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(column_losses).mean()\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "def clean_sentence(dirty_sentence):\n",
    "    cleaning = word_tokenize(dirty_sentence)\n",
    "    \n",
    "    wordlist = [word for word in cleaning if word.isalpha()]\n",
    "    #Removing stopwords didn't seem to help.. check this again after modification?\n",
    "    #wordlist = [word for word in cleaning if (word.isalpha() and not word in stop_words)]\n",
    "    return \" \".join(wordlist)\n",
    "\n",
    "#Right now we're just autocorrecting the words that show up in most used before looking up embeddings\n",
    "#I need to preprocess the entire train/test sets, but this is computationally intensive--  on my TODO list (hoping it'll help too)\n",
    "def autocorrect(potential):\n",
    "    autotry = spellchecker.suggest(re.sub(r'[^\\x00-\\x7f]',r'', potential[0]))\n",
    "    if len(autotry)==0:\n",
    "        return (None, potential[1])\n",
    "                \n",
    "    autocorrect = autotry[0]\n",
    "    if editdistance.eval(potential[0], autocorrect) >3:\n",
    "        return (None, potential[1])\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(autocorrect)\n",
    "    if embedding_vector is not None:\n",
    "        return (embedding_vector, potential[1])\n",
    "    else:\n",
    "        return (None, potential[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95851/95851 [00:11<00:00, 8331.12it/s]\n",
      "100%|██████████| 226998/226998 [00:34<00:00, 6536.26it/s]\n"
     ]
    }
   ],
   "source": [
    "max_words = 30000\n",
    "maxlen = 150\n",
    "\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "clean_train = Parallel(n_jobs=num_cores)(delayed(clean_sentence)(i) for i in tqdm(list_sentences_train))\n",
    "clean_test = Parallel(n_jobs=num_cores)(delayed(clean_sentence)(i) for i in tqdm(list_sentences_test))\n",
    "\n",
    "list_sentences_train = clean_train\n",
    "list_sentences_test = clean_test\n",
    "del(clean_train,clean_test)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "word_index = tokenizer.word_index\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embed_size = 100\n",
    "f = open('./glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2190/2190 [00:07<00:00, 279.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 2190 words not found.\n",
      "We found 936 of these words using autocorrect.\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_words, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "spellchecker = hunspell.HunSpell('./index.dic', './index.aff')\n",
    "notfound = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        notfound.append((word, i))\n",
    "\n",
    "extrafound=0\n",
    "newpairs = Parallel(n_jobs=num_cores)(delayed(autocorrect)(i) for i in tqdm(notfound))\n",
    "for pair in newpairs:\n",
    "    if pair[0] is not None:\n",
    "        extrafound+=1\n",
    "        embedding_matrix[pair[1]] = pair[0]\n",
    "    \n",
    "\n",
    "print(\"There were {:d} words not found.\".format(len(notfound)))\n",
    "print(\"We found {:d} of these words using autocorrect.\".format(extrafound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 150, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 150, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 150, 200)          161600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 75)                15075     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 456       \n",
      "=================================================================\n",
      "Total params: 3,177,131\n",
      "Trainable params: 3,177,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    units = 100\n",
    "    frac_drop = 0.5\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_words, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(75, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "init_weights = model.get_weights()\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',  patience=1, verbose=1, factor=0.5, min_lr=0.001)\n",
    "\n",
    "callbacks_list = [checkpoint, early, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Train/validation split without CV\n",
    "# batch_size = 256\n",
    "# epochs = 5\n",
    "# model.set_weights(init_weights)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_t, y, test_size=0.1, random_state=42)\n",
    "# model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks_list)\n",
    "# logloss = metric(y_valid,model.predict(X_valid, verbose=1))\n",
    "# print(logloss)\n",
    "# model.load_weights(file_path)\n",
    "# y_test = model.predict(X_te, verbose=1)\n",
    "# sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "# sample_submission[list_classes] = y_test\n",
    "# sample_submission.to_csv(\"toxic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 0\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9627Epoch 00001: val_loss improved from inf to 0.05829, saving model to fold_0.hdf5\n",
      "86265/86265 [==============================] - 19s 218us/step - loss: 0.1245 - acc: 0.9627 - val_loss: 0.0583 - val_acc: 0.9790\n",
      "Epoch 2/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9802Epoch 00002: val_loss improved from 0.05829 to 0.05186, saving model to fold_0.hdf5\n",
      "86265/86265 [==============================] - 18s 207us/step - loss: 0.0559 - acc: 0.9802 - val_loss: 0.0519 - val_acc: 0.9805\n",
      "Epoch 3/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9817Epoch 00003: val_loss improved from 0.05186 to 0.04923, saving model to fold_0.hdf5\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0501 - acc: 0.9817 - val_loss: 0.0492 - val_acc: 0.9810\n",
      "Epoch 4/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9829Epoch 00004: val_loss improved from 0.04923 to 0.04909, saving model to fold_0.hdf5\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0463 - acc: 0.9829 - val_loss: 0.0491 - val_acc: 0.9817\n",
      "Epoch 5/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9838Epoch 00005: val_loss improved from 0.04909 to 0.04752, saving model to fold_0.hdf5\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0475 - val_acc: 0.9812\n",
      "Epoch 6/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9846Epoch 00006: val_loss improved from 0.04752 to 0.04682, saving model to fold_0.hdf5\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0401 - acc: 0.9846 - val_loss: 0.0468 - val_acc: 0.9818\n",
      "Epoch 7/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0375 - acc: 0.9854 - val_loss: 0.0476 - val_acc: 0.9818\n",
      "Epoch 8/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0348 - acc: 0.9863 - val_loss: 0.0494 - val_acc: 0.9815\n",
      "Epoch 9/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0325 - acc: 0.9872 - val_loss: 0.0520 - val_acc: 0.9812\n",
      "Epoch 10/10\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9879Epoch 00010: val_loss did not improve\n",
      "86265/86265 [==============================] - 18s 208us/step - loss: 0.0305 - acc: 0.9879 - val_loss: 0.0532 - val_acc: 0.9810\n",
      "9586/9586 [==============================] - 1s 89us/step\n",
      "Loss on fold 0 is 0.046822\n",
      "226998/226998 [==============================] - 14s 61us/step\n",
      "Training on fold 1\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9600Epoch 00001: val_loss improved from inf to 0.06492, saving model to fold_1.hdf5\n",
      "86266/86266 [==============================] - 19s 219us/step - loss: 0.1296 - acc: 0.9601 - val_loss: 0.0649 - val_acc: 0.9778\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9800Epoch 00002: val_loss improved from 0.06492 to 0.05369, saving model to fold_1.hdf5\n",
      "86266/86266 [==============================] - 18s 208us/step - loss: 0.0567 - acc: 0.9800 - val_loss: 0.0537 - val_acc: 0.9808\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9819Epoch 00003: val_loss improved from 0.05369 to 0.05096, saving model to fold_1.hdf5\n",
      "86266/86266 [==============================] - 18s 208us/step - loss: 0.0493 - acc: 0.9818 - val_loss: 0.0510 - val_acc: 0.9813\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9828Epoch 00004: val_loss improved from 0.05096 to 0.05070, saving model to fold_1.hdf5\n",
      "86266/86266 [==============================] - 18s 208us/step - loss: 0.0458 - acc: 0.9828 - val_loss: 0.0507 - val_acc: 0.9818\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9841Epoch 00005: val_loss improved from 0.05070 to 0.04980, saving model to fold_1.hdf5\n",
      "86266/86266 [==============================] - 18s 207us/step - loss: 0.0418 - acc: 0.9841 - val_loss: 0.0498 - val_acc: 0.9820\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9850Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 17s 202us/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0505 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9858- ETA: 5 - ETA: 2s - losEpoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 17s 202us/step - loss: 0.0364 - acc: 0.9858 - val_loss: 0.0512 - val_acc: 0.9815\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9868Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 17s 201us/step - loss: 0.0338 - acc: 0.9868 - val_loss: 0.0562 - val_acc: 0.9817\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9876Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 17s 202us/step - loss: 0.0316 - acc: 0.9876 - val_loss: 0.0554 - val_acc: 0.9809\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9880Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 17s 203us/step - loss: 0.0299 - acc: 0.9880 - val_loss: 0.0570 - val_acc: 0.9814\n",
      "9585/9585 [==============================] - 1s 87us/step\n",
      "Loss on fold 1 is 0.049805\n",
      "226998/226998 [==============================] - 13s 58us/step\n",
      "Training on fold 2\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9606Epoch 00001: val_loss improved from inf to 0.05482, saving model to fold_2.hdf5\n",
      "86266/86266 [==============================] - 18s 213us/step - loss: 0.1242 - acc: 0.9607 - val_loss: 0.0548 - val_acc: 0.9807\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9799Epoch 00002: val_loss improved from 0.05482 to 0.04870, saving model to fold_2.hdf5\n",
      "86266/86266 [==============================] - 18s 207us/step - loss: 0.0565 - acc: 0.9799 - val_loss: 0.0487 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9816Epoch 00003: val_loss improved from 0.04870 to 0.04653, saving model to fold_2.hdf5\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0500 - acc: 0.9816 - val_loss: 0.0465 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9827Epoch 00004: val_loss improved from 0.04653 to 0.04409, saving model to fold_2.hdf5\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0464 - acc: 0.9827 - val_loss: 0.0441 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9837Epoch 00005: val_loss improved from 0.04409 to 0.04387, saving model to fold_2.hdf5\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0426 - acc: 0.9837 - val_loss: 0.0439 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9846Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0400 - acc: 0.9846 - val_loss: 0.0452 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9853Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0375 - acc: 0.9853 - val_loss: 0.0448 - val_acc: 0.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0348 - acc: 0.9863 - val_loss: 0.0470 - val_acc: 0.9837\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9869Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0326 - acc: 0.9869 - val_loss: 0.0502 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9878Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0306 - acc: 0.9878 - val_loss: 0.0509 - val_acc: 0.9825\n",
      "9585/9585 [==============================] - 1s 92us/step\n",
      "Loss on fold 2 is 0.043874\n",
      "226998/226998 [==============================] - 14s 60us/step\n",
      "Training on fold 3\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9498Epoch 00001: val_loss improved from inf to 0.06149, saving model to fold_3.hdf5\n",
      "86266/86266 [==============================] - 19s 226us/step - loss: 0.1514 - acc: 0.9499 - val_loss: 0.0615 - val_acc: 0.9780\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9797Epoch 00002: val_loss improved from 0.06149 to 0.05187, saving model to fold_3.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0577 - acc: 0.9797 - val_loss: 0.0519 - val_acc: 0.9808\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9820Epoch 00003: val_loss improved from 0.05187 to 0.04933, saving model to fold_3.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0500 - acc: 0.9820 - val_loss: 0.0493 - val_acc: 0.9815\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9831Epoch 00004: val_loss improved from 0.04933 to 0.04908, saving model to fold_3.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0460 - acc: 0.9832 - val_loss: 0.0491 - val_acc: 0.9818\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9839Epoch 00005: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0431 - acc: 0.9839 - val_loss: 0.0500 - val_acc: 0.9807\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9845Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0407 - acc: 0.9845 - val_loss: 0.0517 - val_acc: 0.9803\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0379 - acc: 0.9854 - val_loss: 0.0530 - val_acc: 0.9818\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0350 - acc: 0.9863 - val_loss: 0.0520 - val_acc: 0.9815\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9870Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0330 - acc: 0.9870 - val_loss: 0.0541 - val_acc: 0.9818\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9876Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0315 - acc: 0.9876 - val_loss: 0.0556 - val_acc: 0.9812\n",
      "9585/9585 [==============================] - 1s 94us/step\n",
      "Loss on fold 3 is 0.049083\n",
      "226998/226998 [==============================] - 14s 60us/step\n",
      "Training on fold 4\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9638Epoch 00001: val_loss improved from inf to 0.06770, saving model to fold_4.hdf5\n",
      "86266/86266 [==============================] - 19s 219us/step - loss: 0.1244 - acc: 0.9638 - val_loss: 0.0677 - val_acc: 0.9758\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9793Epoch 00002: val_loss improved from 0.06770 to 0.05801, saving model to fold_4.hdf5\n",
      "86266/86266 [==============================] - 18s 210us/step - loss: 0.0581 - acc: 0.9793 - val_loss: 0.0580 - val_acc: 0.9791\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9816Epoch 00003: val_loss improved from 0.05801 to 0.05232, saving model to fold_4.hdf5\n",
      "86266/86266 [==============================] - 18s 213us/step - loss: 0.0508 - acc: 0.9816 - val_loss: 0.0523 - val_acc: 0.9802\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9828Epoch 00004: val_loss improved from 0.05232 to 0.05056, saving model to fold_4.hdf5\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0463 - acc: 0.9828 - val_loss: 0.0506 - val_acc: 0.9804\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9837Epoch 00005: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0429 - acc: 0.9837 - val_loss: 0.0511 - val_acc: 0.9810\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9848Epoch 00006: val_loss improved from 0.05056 to 0.05012, saving model to fold_4.hdf5\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0399 - acc: 0.9848 - val_loss: 0.0501 - val_acc: 0.9808\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9856Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0374 - acc: 0.9856 - val_loss: 0.0531 - val_acc: 0.9815\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 210us/step - loss: 0.0352 - acc: 0.9863 - val_loss: 0.0548 - val_acc: 0.9812\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9870Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 209us/step - loss: 0.0331 - acc: 0.9870 - val_loss: 0.0573 - val_acc: 0.9808\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9878Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 210us/step - loss: 0.0310 - acc: 0.9878 - val_loss: 0.0600 - val_acc: 0.9812\n",
      "9585/9585 [==============================] - 1s 98us/step\n",
      "Loss on fold 4 is 0.050122\n",
      "226998/226998 [==============================] - 14s 61us/step\n",
      "Training on fold 5\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9627Epoch 00001: val_loss improved from inf to 0.05841, saving model to fold_5.hdf5\n",
      "86266/86266 [==============================] - 19s 225us/step - loss: 0.1210 - acc: 0.9628 - val_loss: 0.0584 - val_acc: 0.9798\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9803Epoch 00002: val_loss improved from 0.05841 to 0.05041, saving model to fold_5.hdf5\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0555 - acc: 0.9803 - val_loss: 0.0504 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9819Epoch 00003: val_loss improved from 0.05041 to 0.04846, saving model to fold_5.hdf5\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0496 - acc: 0.9819 - val_loss: 0.0485 - val_acc: 0.9826\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9830Epoch 00004: val_loss improved from 0.04846 to 0.04739, saving model to fold_5.hdf5\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0456 - acc: 0.9830 - val_loss: 0.0474 - val_acc: 0.9834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9838Epoch 00005: val_loss improved from 0.04739 to 0.04729, saving model to fold_5.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0428 - acc: 0.9838 - val_loss: 0.0473 - val_acc: 0.9832\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9846Epoch 00006: val_loss improved from 0.04729 to 0.04535, saving model to fold_5.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0396 - acc: 0.9846 - val_loss: 0.0453 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9853Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0375 - acc: 0.9853 - val_loss: 0.0461 - val_acc: 0.9832\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0347 - acc: 0.9863 - val_loss: 0.0481 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0326 - acc: 0.9872 - val_loss: 0.0491 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9877Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0307 - acc: 0.9877 - val_loss: 0.0508 - val_acc: 0.9824\n",
      "9585/9585 [==============================] - 1s 99us/step\n",
      "Loss on fold 5 is 0.045346\n",
      "226998/226998 [==============================] - 14s 60us/step\n",
      "Training on fold 6\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9643Epoch 00001: val_loss improved from inf to 0.06124, saving model to fold_6.hdf5\n",
      "86266/86266 [==============================] - 19s 221us/step - loss: 0.1184 - acc: 0.9644 - val_loss: 0.0612 - val_acc: 0.9788\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9805Epoch 00002: val_loss improved from 0.06124 to 0.05262, saving model to fold_6.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0550 - acc: 0.9805 - val_loss: 0.0526 - val_acc: 0.9806\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9821Epoch 00003: val_loss improved from 0.05262 to 0.05040, saving model to fold_6.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0493 - acc: 0.9821 - val_loss: 0.0504 - val_acc: 0.9811\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9829Epoch 00004: val_loss improved from 0.05040 to 0.04944, saving model to fold_6.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0453 - acc: 0.9829 - val_loss: 0.0494 - val_acc: 0.9818\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9840Epoch 00005: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0419 - acc: 0.9840 - val_loss: 0.0498 - val_acc: 0.9809\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9846Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0395 - acc: 0.9846 - val_loss: 0.0498 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9857Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0364 - acc: 0.9857 - val_loss: 0.0504 - val_acc: 0.9823\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9865Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0341 - acc: 0.9865 - val_loss: 0.0539 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9873Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0318 - acc: 0.9873 - val_loss: 0.0539 - val_acc: 0.9816\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9881Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 205us/step - loss: 0.0300 - acc: 0.9881 - val_loss: 0.0552 - val_acc: 0.9814\n",
      "9585/9585 [==============================] - 1s 101us/step\n",
      "Loss on fold 6 is 0.049437\n",
      "226998/226998 [==============================] - 14s 60us/step\n",
      "Training on fold 7\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9616Epoch 00001: val_loss improved from inf to 0.05748, saving model to fold_7.hdf5\n",
      "86266/86266 [==============================] - 20s 228us/step - loss: 0.1195 - acc: 0.9617 - val_loss: 0.0575 - val_acc: 0.9797\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9803Epoch 00002: val_loss improved from 0.05748 to 0.04879, saving model to fold_7.hdf5\n",
      "86266/86266 [==============================] - 18s 209us/step - loss: 0.0555 - acc: 0.9804 - val_loss: 0.0488 - val_acc: 0.9821\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9820Epoch 00003: val_loss improved from 0.04879 to 0.04666, saving model to fold_7.hdf5\n",
      "86266/86266 [==============================] - 18s 209us/step - loss: 0.0495 - acc: 0.9820 - val_loss: 0.0467 - val_acc: 0.9828\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9827Epoch 00004: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 209us/step - loss: 0.0458 - acc: 0.9827 - val_loss: 0.0472 - val_acc: 0.9822\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9836Epoch 00005: val_loss improved from 0.04666 to 0.04543, saving model to fold_7.hdf5\n",
      "86266/86266 [==============================] - 18s 209us/step - loss: 0.0424 - acc: 0.9836 - val_loss: 0.0454 - val_acc: 0.9833\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 209us/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0470 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 208us/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0484 - val_acc: 0.9836\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9862Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 210us/step - loss: 0.0349 - acc: 0.9862 - val_loss: 0.0488 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0323 - acc: 0.9872 - val_loss: 0.0516 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9877Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0307 - acc: 0.9877 - val_loss: 0.0538 - val_acc: 0.9830\n",
      "9585/9585 [==============================] - 1s 104us/step\n",
      "Loss on fold 7 is 0.045427\n",
      "226998/226998 [==============================] - 14s 61us/step\n",
      "Training on fold 8\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9663Epoch 00001: val_loss improved from inf to 0.05613, saving model to fold_8.hdf5\n",
      "86266/86266 [==============================] - 20s 234us/step - loss: 0.1195 - acc: 0.9664 - val_loss: 0.0561 - val_acc: 0.9804\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9796Epoch 00002: val_loss improved from 0.05613 to 0.04967, saving model to fold_8.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0568 - acc: 0.9796 - val_loss: 0.0497 - val_acc: 0.9823\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9816Epoch 00003: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0502 - acc: 0.9817 - val_loss: 0.0500 - val_acc: 0.9824\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9826Epoch 00004: val_loss improved from 0.04967 to 0.04731, saving model to fold_8.hdf5\n",
      "86266/86266 [==============================] - 18s 208us/step - loss: 0.0465 - acc: 0.9826 - val_loss: 0.0473 - val_acc: 0.9828\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9836Epoch 00005: val_loss improved from 0.04731 to 0.04663, saving model to fold_8.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0431 - acc: 0.9836 - val_loss: 0.0466 - val_acc: 0.9828\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9844Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0487 - val_acc: 0.9832\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9853Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0375 - acc: 0.9853 - val_loss: 0.0490 - val_acc: 0.9832\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9862Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0350 - acc: 0.9862 - val_loss: 0.0508 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9871Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0328 - acc: 0.9871 - val_loss: 0.0510 - val_acc: 0.9819\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9877Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0310 - acc: 0.9877 - val_loss: 0.0552 - val_acc: 0.9824\n",
      "9585/9585 [==============================] - 1s 106us/step\n",
      "Loss on fold 8 is 0.046630\n",
      "226998/226998 [==============================] - 14s 60us/step\n",
      "Training on fold 9\n",
      "Train on 86266 samples, validate on 9585 samples\n",
      "Epoch 1/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9621Epoch 00001: val_loss improved from inf to 0.05559, saving model to fold_9.hdf5\n",
      "86266/86266 [==============================] - 19s 224us/step - loss: 0.1222 - acc: 0.9622 - val_loss: 0.0556 - val_acc: 0.9806\n",
      "Epoch 2/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9805Epoch 00002: val_loss improved from 0.05559 to 0.04961, saving model to fold_9.hdf5\n",
      "86266/86266 [==============================] - 18s 207us/step - loss: 0.0555 - acc: 0.9805 - val_loss: 0.0496 - val_acc: 0.9821\n",
      "Epoch 3/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9819Epoch 00003: val_loss improved from 0.04961 to 0.04600, saving model to fold_9.hdf5\n",
      "86266/86266 [==============================] - 18s 207us/step - loss: 0.0491 - acc: 0.9819 - val_loss: 0.0460 - val_acc: 0.9830\n",
      "Epoch 4/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9828- Epoch 00004: val_loss improved from 0.04600 to 0.04590, saving model to fold_9.hdf5\n",
      "86266/86266 [==============================] - 18s 208us/step - loss: 0.0459 - acc: 0.9828 - val_loss: 0.0459 - val_acc: 0.9837\n",
      "Epoch 5/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9839Epoch 00005: val_loss improved from 0.04590 to 0.04554, saving model to fold_9.hdf5\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0426 - acc: 0.9839 - val_loss: 0.0455 - val_acc: 0.9835\n",
      "Epoch 6/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9846Epoch 00006: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0400 - acc: 0.9846 - val_loss: 0.0457 - val_acc: 0.9833\n",
      "Epoch 7/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9855Epoch 00007: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 206us/step - loss: 0.0373 - acc: 0.9855 - val_loss: 0.0471 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9864Epoch 00008: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0347 - acc: 0.9864 - val_loss: 0.0494 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9870Epoch 00009: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 211us/step - loss: 0.0330 - acc: 0.9870 - val_loss: 0.0491 - val_acc: 0.9830\n",
      "Epoch 10/10\n",
      "86016/86266 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9878Epoch 00010: val_loss did not improve\n",
      "86266/86266 [==============================] - 18s 212us/step - loss: 0.0309 - acc: 0.9878 - val_loss: 0.0526 - val_acc: 0.9833\n",
      "9585/9585 [==============================] - 1s 111us/step\n",
      "Loss on fold 9 is 0.045540\n",
      "226998/226998 [==============================] - 14s 62us/step\n",
      "CV loss is 0.047209\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 10\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, random_state=42)\n",
    "y_test = np.zeros((len(X_te),6))\n",
    "valpred = np.zeros((len(X_t),6))\n",
    "for j, (train_index, val_index) in enumerate(kf.split(X_t)):\n",
    "    print(\"Training on fold {:d}\".format(j))\n",
    "    #model.set_weights(init_weights)\n",
    "    model = get_model()\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',  patience=20, verbose=1, factor=0.5, min_lr=0.005)\n",
    "    checkpoint = ModelCheckpoint(\"fold_\" + str(j) + \".hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    model.fit(X_t[train_index], y[train_index], batch_size=batch_size, epochs=epochs, validation_data=(X_t[val_index], y[val_index]), callbacks=[checkpoint, learning_rate_reduction])\n",
    "    \n",
    "    model.load_weights(\"fold_\" + str(j) + \".hdf5\")\n",
    "    \n",
    "    valpred[val_index,:]=model.predict(X_t[val_index], verbose=1, batch_size=512)\n",
    "    logloss = metric(y[val_index],valpred[val_index,:])\n",
    "    print(\"Loss on fold {:d} is {:f}\".format(j,logloss))\n",
    "    y_test += model.predict(X_te, verbose=1, batch_size=512)\n",
    "    \n",
    "y_test = y_test/num_folds\n",
    "logloss = metric(y,valpred)\n",
    "print(\"CV loss is {:f}\".format(logloss))\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "temp = pd.read_csv(\"./train.csv\")\n",
    "temp[list_classes] = valpred\n",
    "temp.to_csv(\"validation_predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"./train.csv\")\n",
    "temp[list_classes] = valpred\n",
    "temp.to_csv(\"validation_predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
