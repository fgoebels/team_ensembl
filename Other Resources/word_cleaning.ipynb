{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import hunspell\n",
    "import editdistance #Levenshtein distance\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./glove.840B.300d.txt') #Use this as our list of valid words\n",
    "valid_words = []\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    #coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    valid_words.append(word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "spellchecker = hunspell.HunSpell('./index.dic', './index.aff')\n",
    "\n",
    "f = open(\"badwords.txt\", \"r\") #Using a dictionary for duplicates (fuck vs fuk vs f_u_c_k) might be better\n",
    "badwords = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "word_set = set(valid_words + badwords) #Much faster searching, add the badwords too.\n",
    "del(valid_words) #clean up\n",
    "regex_url = re.compile(r'''http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''')\n",
    "regex_email = re.compile(r'''(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])''')\n",
    "regex_underscorelink = re.compile(r'\\[(.*)_link:.*\\]',re.IGNORECASE)\n",
    "regex_tags = re.compile(r'\\[\\[(category|wikipedia|user).*\\]\\]')\n",
    "regex_strayapos = re.compile(r'''((?<=[^a-zA-Z])'|'(?![a-zA-Z]))''')\n",
    "regex_nums = re.compile(r'\\d+')\n",
    "regex_notascii = re.compile(r'[^\\x00-\\x7f]')\n",
    "punct = '''!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~'''\n",
    "\n",
    "def clean_sentence(dirty_sentence):\n",
    "    dirty_sentence = dirty_sentence.lower() #Lowercase\n",
    "    dirty_sentence = regex_underscorelink.sub('', dirty_sentence) #Remove everything inside [wiki_link: ...] etc\n",
    "    dirty_sentence = regex_tags.sub('', dirty_sentence) #Remove some extra tags\n",
    "    dirty_sentence = regex_url.sub('', dirty_sentence) #Remove URLs\n",
    "    dirty_sentence = regex_email.sub('', dirty_sentence) #Remove E-mails\n",
    "    dirty_sentence = regex_nums.sub('', dirty_sentence) #Remove numbers    \n",
    "    dirty_sentence = regex_notascii.sub('', dirty_sentence) #Remove some non-ASCII (hunspell has some difficulties otherwise)\n",
    "    dirty_sentence = regex_strayapos.sub('', dirty_sentence) #Remove stray apostrophes\n",
    "    dirty_sentence = dirty_sentence.translate(str.maketrans({a:None for a in punct})) #Remove punctuation, except apostrophe\n",
    "    cleaning = word_tokenize(dirty_sentence)\n",
    "    \n",
    "    wordlist = []\n",
    "    for word in cleaning:            \n",
    "        if word in word_set: #If this is a word, add it\n",
    "            wordlist.append(word)\n",
    "        else:\n",
    "            autotry = spellchecker.suggest(word) #Try to correct word\n",
    "            if len(autotry)==0:\n",
    "                wordlist.append(word) #No suggestions found, add it as-is\n",
    "                continue\n",
    "            \n",
    "            autocorrect = autotry[0] #Take the first suggestion\n",
    "            if editdistance.eval(word, autocorrect) >3: #Suggestion looks too different from word, leave it alone and add\n",
    "                wordlist.append(word)\n",
    "                continue\n",
    "            \n",
    "            wordlist.append(autocorrect) #Replace word with our suggested word\n",
    "    \n",
    "    return \" \".join(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 95851 sentences\n",
      "done cleaning\n",
      "starting to save\n"
     ]
    }
   ],
   "source": [
    "#Can do: tail -f log.txt\n",
    "#To monitor progress\n",
    "f = open(\"log.txt\", \"w\")\n",
    "print(\"Cleaning {:d} sentences\".format(len(list_sentences_train)))\n",
    "with redirect_stdout(f):\n",
    "    clean_train = Parallel(n_jobs=num_cores, verbose=50, batch_size = 2)(delayed(clean_sentence)(i) for i in list_sentences_train)\n",
    "f.close()\n",
    "\n",
    "train[\"comment_text\"]=clean_train\n",
    "train.to_csv(\"train_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 226998 sentences\n"
     ]
    }
   ],
   "source": [
    "f = open(\"log.txt\", \"w\")\n",
    "print(\"Cleaning {:d} sentences\".format(len(list_sentences_test)))\n",
    "with redirect_stdout(f):\n",
    "    clean_test = Parallel(n_jobs=num_cores, verbose=50, batch_size = 2)(delayed(clean_sentence)(i) for i in list_sentences_test)\n",
    "f.close()\n",
    "\n",
    "test[\"comment_text\"]=clean_test\n",
    "test.to_csv(\"test_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
