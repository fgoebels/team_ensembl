{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "import hunspell\n",
    "import re, os, io\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "num_cores = mp.cpu_count()\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import editdistance\n",
    "\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./glove.840B.300d.txt') #Use this as our list of valid words\n",
    "valid_words = []\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    #coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    valid_words.append(word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spellchecker = hunspell.HunSpell('./index.dic', './index.aff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"bad-words.txt\", \"r\") #Using a dictionary for duplicates (fuck vs fuk vs f_u_c_k) might be better\n",
    "badwords = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_set = set(valid_words + badwords) #Much faster searching, add the badwords too.\n",
    "del(valid_words) #clean up\n",
    "\n",
    "\n",
    "regex_notascii = re.compile(r'[^\\x00-\\x7f]')\n",
    "\n",
    "# changed .* to .*? to make the regex non greedy\n",
    "regex_underscorelink = re.compile(r'\\[(.*?)_link:.*?\\]',re.IGNORECASE)\n",
    "# changed .* to .*? to make the regex non greedy\n",
    "regex_tags = re.compile(r'\\[\\[(category|wikipedia|user).*?\\]\\]')\n",
    "\n",
    "#added # since some urls have hashtags at the end\n",
    "regex_url = re.compile(r'''http[s]?://(?:[a-zA-Z]|[0-9]|[#$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''')\n",
    "regex_email = re.compile(r'''(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])''')\n",
    "\n",
    "# added some regex to recove swarwords\n",
    "regex_dc_shit = re.compile(r's(h|\\*)(i|\\*)(t|\\*)')\n",
    "regex_dc_fuck = re.compile(r'f(u|\\*)(c|\\*)(k|\\*)')\n",
    "regex_dc_dick = re.compile(r'd(i|\\*)(c|\\*)(k|\\*)')\n",
    "regex_dc_ass = re.compile(r'a(s|\\*)(s|\\*)')\n",
    "regex_dc_cunt = re.compile(r'c(u|\\*)(n|\\*)(t|\\*)')\n",
    "regex_dc_bitch = re.compile(r'b(i|\\*)(t|\\*)(c|\\*)(h|\\*)')\n",
    "\n",
    "regex_strayapos = re.compile(r'''((?<=[^a-zA-Z])'|'(?![a-zA-Z]))''')\n",
    "regex_nums = re.compile(r'\\d+')\n",
    "punct = '''!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~'''\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(dirty_sentence):\n",
    "    dirty_sentence = dirty_sentence.lower() #Lowercase\n",
    "    dirty_sentence = regex_underscorelink.sub(' ', dirty_sentence) #Remove everything inside [wiki_link: ...] etc\n",
    "    dirty_sentence = regex_tags.sub(' ', dirty_sentence) #Remove some extra tags\n",
    "    dirty_sentence = regex_url.sub(' ', dirty_sentence) #Remove URLs\n",
    "    dirty_sentence = regex_email.sub(' ', dirty_sentence) #Remove E-mails\n",
    "    dirty_sentence = regex_nums.sub(' ', dirty_sentence) #Remove numbers    \n",
    "    dirty_sentence = regex_notascii.sub(' ', dirty_sentence) #Remove some non-ASCII (hunspell has some difficulties otherwise)\n",
    "    dirty_sentence = regex_strayapos.sub(' ', dirty_sentence) #Remove stray apostrophes\n",
    "    \n",
    "    # recovering bad words\n",
    "    # only applicable in fringe cases ? mabe remove later on\n",
    "    dirty_sentence = regex_dc_shit.sub('shit', dirty_sentence)\n",
    "    dirty_sentence = regex_dc_fuck.sub('fuck', dirty_sentence)\n",
    "    dirty_sentence = regex_dc_dick.sub('dick', dirty_sentence)\n",
    "    dirty_sentence = regex_dc_ass.sub('ass', dirty_sentence)\n",
    "    dirty_sentence = regex_dc_cunt.sub('cunt', dirty_sentence)\n",
    "    dirty_sentence = regex_dc_bitch.sub('bitch', dirty_sentence)\n",
    "    \n",
    "    \n",
    "    dirty_sentence = re.sub(r\"[^A-Za-z']\", \" \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"what's\", \"what is \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"\\'s\", \" \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"\\'ve\", \" have \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"n't\", \" not \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"i'm\", \"i am \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"'m\", \" am \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"\\'re\", \" are \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"\\'d\", \" would \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"\\'ll\", \" will \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\" e g \", \" eg \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\" b g \", \" bg \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\" u s \", \" american \", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"e - mail\", \"email\", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"j k\", \"jk\", dirty_sentence)\n",
    "    dirty_sentence = re.sub(r\"\\s{2,}\", \" \", dirty_sentence)\n",
    "    \n",
    "    cleaning = word_tokenize(dirty_sentence)\n",
    "    \n",
    "    \n",
    "    wordlist = []\n",
    "    for word in cleaning:            \n",
    "        if word in word_set: #If this is a word, add it\n",
    "            wordlist.append(word)\n",
    "        else:\n",
    "            autotry = spellchecker.suggest(word) #Try to correct word\n",
    "            if len(autotry)==0:\n",
    "                wordlist.append(word) #No suggestions found, add it as-is\n",
    "                continue\n",
    "            \n",
    "            autocorrect = autotry[0] #Take the first suggestion\n",
    "            if editdistance.eval(word, autocorrect) >3: #Suggestion looks too different from word, leave it alone and add\n",
    "                wordlist.append(word)\n",
    "                continue\n",
    "            \n",
    "            wordlist.append(autocorrect) #Replace word with our suggested word\n",
    "    \n",
    "    # remove stop words like I, the, for, ...\n",
    "    #wordlist = [w for w in wordlist if not w in stops]\n",
    "    \n",
    "    #stemming words running -> run\n",
    "    #wordlist = [stemmer.stem(word) for word in wordlist]\n",
    "    \n",
    "    return \" \".join(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summaries fuck i am we will great'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence(\"summaries fuck I'm we'll great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [56:48<00:00, 46.82it/s]  \n"
     ]
    }
   ],
   "source": [
    "clean_train = ['']*len(list_sentences_train)\n",
    "for i in tqdm(range(len(list_sentences_train))):\n",
    "    clean_train[i] = clean_sentence(list_sentences_train[i])\n",
    "    \n",
    "\n",
    "train[\"comment_text\"]=clean_train\n",
    "train.to_csv(\"train_cleaned_unfiltered.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [1:08:09<00:00, 32.96it/s]           \n"
     ]
    }
   ],
   "source": [
    "clean_test = ['']*len(list_sentences_test)\n",
    "for i in tqdm(range(len(list_sentences_test))):\n",
    "    clean_test[i] = clean_sentence(list_sentences_test[i])\n",
    "    \n",
    "\n",
    "test[\"comment_text\"]=clean_test\n",
    "test.to_csv(\"test_cleaned_unfiltered.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
