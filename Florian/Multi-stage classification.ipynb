{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import words, stopwords\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "import gensim.models.word2vec as w2v\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import maketrans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train_cleaned.csv\")\n",
    "test = pd.read_csv(\"./test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text):\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"'m\", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/fgoebels/workspace/kaggle-toxicity/train.csv\")\n",
    "test = pd.read_csv(\"/Users/fgoebels/workspace/kaggle-toxicity/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 8)\n",
      "(226998, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318321/318321 [00:23<00:00, 13303.27it/s]\n"
     ]
    }
   ],
   "source": [
    "all_text = test[\"comment_text\"].dropna().values\n",
    "all_text = list(np.append(all_text, train[\"comment_text\"].dropna().values))\n",
    "all_text_clean = [\"\"]*len(all_text)\n",
    "for i in tqdm(range(len(all_text))):\n",
    "    all_text_clean[i] = text_to_wordlist(all_text[i])\n",
    "all_text_tokenized = [word_tokenize(s) for s in all_text_clean]\n",
    "#print len(all_text)\n",
    "#print len(all_text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters of the word2vec model\n",
    "num_features = 300 # dimensions of each word embedding\n",
    "min_word_count = 1 # this is not advisable but since we need to extract\n",
    "# feature vector for each word we need to do this\n",
    "num_workers = mp.cpu_count() # number of threads running in parallel\n",
    "context_size = 7 # context window length\n",
    "downsampling = 1e-3 # downsampling for very frequent words\n",
    "seed = 1 # seed for random number generator to make results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = w2v.Word2Vec(\n",
    "    sg = 1, seed = seed,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context_size,\n",
    "    sample = downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to built the vocab\n",
    "word2vec_.build_vocab(all_text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72449580"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.train(all_text_tokenized, total_examples = word2vec_.corpus_count, epochs = word2vec_.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 0.7587818503379822),\n",
       " ('females', 0.602188229560852),\n",
       " ('heterosexual', 0.60048907995224),\n",
       " ('genitalia', 0.5827293395996094),\n",
       " ('senility', 0.5809351205825806),\n",
       " ('males', 0.5778359174728394),\n",
       " ('femininity', 0.5775822401046753),\n",
       " ('transsexual', 0.5729291439056396),\n",
       " ('nohibokoma', 0.5721511244773865),\n",
       " ('nohiboka', 0.570717453956604)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.wv.most_similar('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('metalpunkhardcore', 0.9332546591758728),\n",
       " ('luved', 0.9314939975738525),\n",
       " ('homofobe', 0.9314460754394531),\n",
       " ('flexin', 0.9312250018119812),\n",
       " ('nazorean', 0.9306169748306274),\n",
       " ('wahahhahaahah', 0.9297462701797485),\n",
       " ('giddily', 0.9291085600852966),\n",
       " ('underoos', 0.9285200834274292),\n",
       " ('fnidoasnfdklasnfeiowanfdsznvjdsafneiawnmdxsancjkdsafndsakjfeniwaofds',\n",
       "  0.9280965924263),\n",
       " ('saidit', 0.9279337525367737)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.wv.most_similar('lolly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ullmann', 0.6498888731002808),\n",
       " ('cline', 0.6433099508285522),\n",
       " ('suck', 0.6330228447914124),\n",
       " ('syndrom', 0.6034367680549622),\n",
       " ('ballsack', 0.5976033210754395),\n",
       " ('whore', 0.5921552181243896),\n",
       " ('dickheadfuck', 0.5883691906929016),\n",
       " ('douchebag', 0.5871796011924744),\n",
       " ('faggoty', 0.5869563221931458),\n",
       " ('sucking', 0.5817825794219971)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.wv.most_similar('dick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 0.8036949634552002),\n",
       " ('females', 0.6898893117904663),\n",
       " ('males', 0.6633477807044983),\n",
       " ('heterosexual', 0.6574035882949829),\n",
       " ('femininity', 0.6503486633300781),\n",
       " ('senility', 0.6500623226165771),\n",
       " ('genitalia', 0.647432804107666),\n",
       " ('sexually', 0.6322382092475891),\n",
       " ('hetero', 0.6257021427154541),\n",
       " ('woman', 0.625482439994812)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.wv.most_similar('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lolooolbootstoots', 0.6720337867736816),\n",
       " ('friggen', 0.6649272441864014),\n",
       " ('bunksteve', 0.6477774977684021),\n",
       " ('dashiel', 0.6474275588989258),\n",
       " ('cody', 0.6383813619613647),\n",
       " ('cline', 0.6378067135810852),\n",
       " ('homosexual', 0.6349483728408813),\n",
       " ('cyphoidbomb', 0.6246486902236938),\n",
       " ('loll', 0.622316300868988),\n",
       " ('lesbian', 0.6129813194274902)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.wv.most_similar('gay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suck', 0.7001449465751648),\n",
       " ('ullmann', 0.66269850730896),\n",
       " ('cline', 0.6521153450012207),\n",
       " ('cock', 0.6502227783203125),\n",
       " ('ballsack', 0.6378535032272339),\n",
       " ('faggoty', 0.6336411833763123),\n",
       " ('whore', 0.631374180316925),\n",
       " ('laurent', 0.6251245141029358),\n",
       " ('sucking', 0.6240524053573608),\n",
       " ('chode', 0.6206081509590149)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.wv.most_similar('dick')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
