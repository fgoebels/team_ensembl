{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "import gensim.models.word2vec as w2v\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import maketrans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train_cleaned_no-stopwords.csv\")\n",
    "test = pd.read_csv(\"./test_cleaned_no-stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n",
      "(153164, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 8946/159571 [00:00<00:01, 89404.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203883\n",
      "101897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:01<00:00, 90441.86it/s]\n",
      "100%|██████████| 153164/153164 [00:01<00:00, 96132.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_words(corpus):\n",
    "    word_counts = {}\n",
    "    for sentence in corpus:\n",
    "        for word in set(sentence.split()):\n",
    "                if word not in word_counts: word_counts[word] = 0\n",
    "                word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "def remove_low_freq_words(text, val_w):\n",
    "    text = set(text.split())\n",
    "    text = list(text & val_w)\n",
    "    if len(text) == 0:\n",
    "        text = [\"CVxTz\"]\n",
    "    text = \" \".join(text)    \n",
    "    return text\n",
    "        \n",
    "all_text = list(list_sentences_train) + list(list_sentences_test)\n",
    "print len(all_text)\n",
    "word_counts = count_words(all_text)\n",
    "print len(word_counts)\n",
    "\n",
    "val_words = set()\n",
    "for k, v in word_counts.items():\n",
    "    if v > 1:\n",
    "        val_words.add(k)\n",
    "print(len(val_words))\n",
    "list_sentences_f_train  = [\"CVxTz\"] * len(list_sentences_train)\n",
    "list_sentences_f_test   = [\"CVxTz\"] * len(list_sentences_test)\n",
    "\n",
    "for i in tqdm(range(len(list_sentences_train))):\n",
    "    list_sentences_f_train[i] = remove_low_freq_words(list_sentences_train[i], val_words)\n",
    "\n",
    "for i in tqdm(range(len(list_sentences_test))):\n",
    "    list_sentences_f_test[i] = remove_low_freq_words(list_sentences_test[i], val_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98821 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Generate common embedings\n",
    "tokenizer = text.Tokenizer(num_words=len(val_words))\n",
    "tokenizer.fit_on_texts(list(list_sentences_f_train) + list(list_sentences_f_test))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_f_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_f_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "list_senteces_all = list_sentences_f_train + list_sentences_f_test\n",
    "for i in tqdm(range(len(list_senteces_all))):\n",
    "    list_senteces_all[i] = list_senteces_all[i].split(\" \")\n",
    "    \n",
    "# hyper parameters of the word2vec model\n",
    "num_features = 300 # dimensions of each word embedding\n",
    "min_word_count = 1 # this is not advisable but since we need to extract\n",
    "# feature vector for each word we need to do this\n",
    "num_workers = mp.cpu_count() # number of threads running in parallel\n",
    "context_size = 7 # context window length\n",
    "downsampling = 1e-3 # downsampling for very frequent words\n",
    "seed = 1 # seed for random number generator to make results reproducible\n",
    "\n",
    "word2vec_ = w2v.Word2Vec(\n",
    "    sg = 1, seed = seed,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context_size,\n",
    "    sample = downsampling\n",
    ")\n",
    "\n",
    "word2vec_.build_vocab(list_senteces_all)\n",
    "word2vec_.train(list_senteces_all, total_examples = word2vec_.corpus_count, epochs = 10)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i ==0: print word\n",
    "    if word in word2vec_.wv:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = word2vec_.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3319,\n",
       " 12906,\n",
       " 46,\n",
       " 8,\n",
       " 63,\n",
       " 2376,\n",
       " 301,\n",
       " 37,\n",
       " 228,\n",
       " 15192,\n",
       " 652,\n",
       " 572,\n",
       " 2782,\n",
       " 964,\n",
       " 1122,\n",
       " 2319,\n",
       " 4804,\n",
       " 10037,\n",
       " 5743,\n",
       " 39,\n",
       " 129,\n",
       " 2,\n",
       " 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312735/312735 [00:01<00:00, 194910.22it/s]\n"
     ]
    }
   ],
   "source": [
    "list_senteces_all = list_sentences_f_train + list_sentences_f_test\n",
    "for i in tqdm(range(len(list_senteces_all))):\n",
    "    list_senteces_all[i] = list_senteces_all[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retired',\n",
       " 'vandalisms',\n",
       " 'since',\n",
       " 'please',\n",
       " 'edits',\n",
       " 'voted',\n",
       " 'template',\n",
       " 'new',\n",
       " 'reverted',\n",
       " 'dolls',\n",
       " 'username',\n",
       " 'explanation',\n",
       " 'gas',\n",
       " 'fan',\n",
       " 'york',\n",
       " 'fac',\n",
       " 'hardcore',\n",
       " 'metallica',\n",
       " 'closure',\n",
       " 'made',\n",
       " 'remove',\n",
       " 'page',\n",
       " 'talk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_senteces_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters of the word2vec model\n",
    "num_features = 300 # dimensions of each word embedding\n",
    "min_word_count = 1 # this is not advisable but since we need to extract\n",
    "# feature vector for each word we need to do this\n",
    "num_workers = mp.cpu_count() # number of threads running in parallel\n",
    "context_size = 7 # context window length\n",
    "downsampling = 1e-3 # downsampling for very frequent words\n",
    "seed = 1 # seed for random number generator to make results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = w2v.Word2Vec(\n",
    "    sg = 1, seed = seed,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context_size,\n",
    "    sample = downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to built the vocab\n",
    "word2vec_.build_vocab(list_senteces_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78184630, 79928320)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_.train(list_senteces_all, total_examples = word2vec_.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Zoroastrianism', 'monotheistic', 'shias', 'ilaha', 'apostasy', 'tabari', 'idolatry', \"shi'as\", 'religious', 'wickedness'])\n"
     ]
    }
   ],
   "source": [
    "print word2vec_.wv.most_similar('islam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i ==0: print word\n",
    "    if word in word2vec_.wv:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = word2vec_.wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "max_features = 98822\n",
    "embed_size = 300\n",
    "maxlen = 150\n",
    "\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 191s 1ms/step - loss: 0.0427 - acc: 0.9836\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 191s 1ms/step - loss: 0.0378 - acc: 0.9850\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 191s 1ms/step - loss: 0.0336 - acc: 0.9866\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0303 - acc: 0.9880\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0272 - acc: 0.9893\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0245 - acc: 0.9904\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0220 - acc: 0.9915\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0194 - acc: 0.9925\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0171 - acc: 0.9934\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0149 - acc: 0.9944\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0130 - acc: 0.9951\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0113 - acc: 0.9957\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0101 - acc: 0.9963\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0091 - acc: 0.9967\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0084 - acc: 0.9969\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0076 - acc: 0.9972\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0069 - acc: 0.9975\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0058 - acc: 0.9978\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0055 - acc: 0.9979\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=512, epochs=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_p = model.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_p = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.column_stack([train[\"id\"].values, X_t_p])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/train_1803_FG_LSTMmulticlass.csv\", header= False, index= False)\n",
    "pd.DataFrame(np.column_stack([test[\"id\"].values, X_te_p])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/test_1803_FG_LSTMmulticlass.csv\", header= False, index= False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
