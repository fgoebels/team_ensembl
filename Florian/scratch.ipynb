{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from  keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import copy\n",
    "\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train_cleaned_no-stopwords.csv\")\n",
    "test = pd.read_csv(\"./test_cleaned_no-stopwords.csv\")\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312735\n",
      "203883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9048/159571 [00:00<00:01, 90433.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:01<00:00, 90783.46it/s]\n",
      "100%|██████████| 153164/153164 [00:01<00:00, 97668.36it/s] \n"
     ]
    }
   ],
   "source": [
    "def count_words(corpus):\n",
    "    word_counts = {}\n",
    "    for sentence in corpus:\n",
    "        for word in set(sentence.split()):\n",
    "                if word not in word_counts: word_counts[word] = 0\n",
    "                word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "def remove_low_freq_words(text, val_w):\n",
    "    text = set(text.split())\n",
    "    text = list(text & val_w)\n",
    "    if len(text) == 0:\n",
    "        text = [\"CVxTz\"]\n",
    "    text = \" \".join(text)    \n",
    "    return text\n",
    "        \n",
    "all_text = list(list_sentences_train) + list(list_sentences_test)\n",
    "print len(all_text)\n",
    "word_counts = count_words(all_text)\n",
    "print len(word_counts)\n",
    "\n",
    "val_words = set()\n",
    "for k, v in word_counts.items():\n",
    "    if v > 1:\n",
    "        val_words.add(k)\n",
    "print(len(val_words))\n",
    "list_sentences_f_train  = [\"CVxTz\"] * len(list_sentences_train)\n",
    "list_sentences_f_test   = [\"CVxTz\"] * len(list_sentences_test)\n",
    "\n",
    "for i in tqdm(range(len(list_sentences_train))):\n",
    "    list_sentences_f_train[i] = remove_low_freq_words(list_sentences_train[i], val_words)\n",
    "\n",
    "for i in tqdm(range(len(list_sentences_test))):\n",
    "    list_sentences_f_test[i] = remove_low_freq_words(list_sentences_test[i], val_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s): return s.split(\" \")\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "\n",
    "train_tfidf = vec.fit_transform(list_sentences_f_train)\n",
    "test_tfidf = vec.transform(list_sentences_f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0]/batch_size\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_multiclass_clf:       \n",
    "    def __init__(self):\n",
    "        self.model = \"\"\n",
    "    \n",
    "    def get_mdl(self, X, y):\n",
    "        adam = Adam(lr=0.001)\n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(input_shape=(X.shape[1],)))\n",
    "        model.add(Dense(64, activation='sigmoid'))\n",
    "        model.add(Dropout(rate=0.6))\n",
    "        model.add(Dense(2048, activation='sigmoid'))\n",
    "        model.add(Dropout(rate=0.6))\n",
    "        #self.model.add(Dense(2048, activation='sigmoid'))\n",
    "        #self.model.add(Dropout(rate=0.6))\n",
    "        #self.model.add(Dense(2048, activation='sigmoid'))\n",
    "        #self.model.add(Dropout(rate=0.6))\n",
    "        if len(y.shape) > 1:\n",
    "            model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "            model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "        else:\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        #print(self.model.summary())\n",
    "        batch_size = 512\n",
    "        model.fit_generator(generator=batch_generator(X, y, batch_size, True),\n",
    "                            nb_epoch=1,\n",
    "                            samples_per_epoch=X.shape[0]/batch_size)\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.get_mdl(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_1vsall_clf:\n",
    "    \n",
    "        \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "    \n",
    "    def get_mdl(self, X, y):\n",
    "        adam = Adam(lr=0.001)\n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(input_shape=(X.shape[1],)))\n",
    "        model.add(Dense(64, activation='sigmoid'))\n",
    "        model.add(Dropout(rate=0.6))\n",
    "        model.add(Dense(2048, activation='sigmoid'))\n",
    "        model.add(Dropout(rate=0.6))\n",
    "        #self.model.add(Dense(2048, activation='sigmoid'))\n",
    "        #self.model.add(Dropout(rate=0.6))\n",
    "        #self.model.add(Dense(2048, activation='sigmoid'))\n",
    "        #self.model.add(Dropout(rate=0.6))\n",
    "        if len(y.shape) > 1:\n",
    "            model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "            model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "        else:\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        #print(self.model.summary())\n",
    "        batch_size = 512\n",
    "        model.fit_generator(generator=batch_generator(X, y, batch_size, True),\n",
    "                            nb_epoch=10,\n",
    "                            samples_per_epoch=X.shape[0]/batch_size)\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            for i in range(y.shape[1]):\n",
    "                m = self.get_mdl(X, y[:,i])\n",
    "                self.models.append((m))\n",
    "        else:\n",
    "            m = self.get_mdl(X, y)\n",
    "            self.models.append((m))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], len(self.models)))\n",
    "        for i in range(len(self.models)):\n",
    "            m = self.models[i]\n",
    "            preds[:,i] = m.predict(X).flatten()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB_LR:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.r  =[]\n",
    "    \n",
    "    def pr(self, x, y_i, y):\n",
    "        p = x[y==y_i].sum(0)\n",
    "        return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "    def get_mdl(self, X, y):\n",
    "        #y = y.values\n",
    "        r = np.log(self.pr(X, 1,y) / self.pr(X, 0,y))\n",
    "        m = LogisticRegression(C=10, dual=True)\n",
    "        #m = SVC(kernel='sigmoid', probability = True)\n",
    "        x_nb = X.multiply(r)\n",
    "        return m.fit(x_nb, y), r\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            for i in range(y.shape[1]):\n",
    "                m,r = self.get_mdl(X, y[:,i])\n",
    "                self.models.append((m, r))\n",
    "        else:\n",
    "            m,r = self.get_mdl(X, y)\n",
    "            self.models.append((m, r))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], len(self.models)))\n",
    "        for i in range(len(self.models)):\n",
    "            m, r = self.models[i]\n",
    "            preds[:,i] = m.predict_proba(X.multiply(r))[:,1]\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stepwise_clf:\n",
    "    \n",
    "    \n",
    "    def __init__(self, y):\n",
    "        self.y_all = y\n",
    "        self.y_toxic_or_not = y.max(axis=1)\n",
    "        \n",
    "    def fit(self, model, X):\n",
    "        self.model_all = model()\n",
    "        self.model_toxic_or_not = model()\n",
    "        print \"fitting all\"\n",
    "        self.model_all.fit(X, self.y_all)\n",
    "        print \"fitting toxic or not\"\n",
    "        self.model_toxic_or_not.fit(X, self.y_toxic_or_not)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (self.model_all.predict(X), self.model_toxic_or_not.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def get_mdl(self, X, y):\n",
    "        m = LogisticRegression(C=10, dual=True)\n",
    "        return m.fit(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            for i in range(y.shape[1]):\n",
    "                m = self.get_mdl(X, y[:,i])\n",
    "                self.models.append((m))\n",
    "        else:\n",
    "            m = self.get_mdl(X, y)\n",
    "            self.models.append((m))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], len(self.models)))\n",
    "        for i in range(len(self.models)):\n",
    "            m = self.models[i]\n",
    "            preds[:,i] = m.predict_proba(X)[:,1]\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "class xgboost:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.xgb_params = {'eta': 0.3, \n",
    "              'max_depth': 5, \n",
    "              'subsample': 0.8, \n",
    "              'colsample_bytree': 0.8, \n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc', \n",
    "              'seed': 23,\n",
    "              'n_estimators':1000,\n",
    "              'nthread' : 4\n",
    "             }\n",
    "        self.num_steps = 200\n",
    "        \n",
    "    def get_mdl(self, X, y):\n",
    "        dtrain = xgb.DMatrix( X, label = y)\n",
    "        m = xgb.train(self.xgb_params, dtrain, self.num_steps)\n",
    "        return m\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            for i in range(y.shape[1]):\n",
    "                m = self.get_mdl(X, y[:,i])\n",
    "                self.models.append((m))\n",
    "        else:\n",
    "            m = self.get_mdl(X, y)\n",
    "            self.models.append((m))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], len(self.models)))\n",
    "        for i in range(len(self.models)):\n",
    "            m = self.models[i]\n",
    "            preds[:,i] = m.predict(xgb.DMatrix(X))\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class randomforest:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def get_mdl(self, X, y):\n",
    "        m = RandomForestClassifier(n_estimators=100, n_jobs=4)\n",
    "        return m.fit(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            for i in range(y.shape[1]):\n",
    "                print i\n",
    "                m = self.get_mdl(X, y[:,i])\n",
    "                self.models.append((m))\n",
    "        else:\n",
    "            m = self.get_mdl(X, y)\n",
    "            self.models.append((m))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], len(self.models)))\n",
    "        for i in range(len(self.models)):\n",
    "            m = self.models[i]\n",
    "            preds[:,i] = m.predict_proba(X)[:,1]\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_tfidf, y, test_size=0.33)\n",
    "\n",
    "all_models = {\"nb_lr\" : NB_LR, \"gboost\" :xgboost }\n",
    "\n",
    "for name, m in all_models.items():\n",
    "    print name\n",
    "    setwiser = stepwise_clf(y_train)\n",
    "    setwiser.fit(m, X_train)\n",
    "\n",
    "    X_val_lvl1 = list(setwiser.predict(X_val))\n",
    "    X_train_lvl1 = list(setwiser.predict(X_train))\n",
    "\n",
    "    #for sel in [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1],[1,1,1]]:\n",
    "    for sel in [[1,0],[0,1],[1,1]]:\n",
    "        sel = np.array(sel)\n",
    "        X_train_stacked = np.column_stack([X_train_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        X_val_stacked = np.column_stack([X_val_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "\n",
    "        print(X_train_stacked.shape)\n",
    "        print(X_val_stacked.shape)\n",
    "\n",
    "        from scipy import sparse\n",
    "        l = LogisticRegression(C=10, dual=True)\n",
    "        lr = LR()\n",
    "        lr.fit(X_train_stacked, y_train)\n",
    "        pred = lr.predict(X_val_stacked)\n",
    "        print(roc_auc_score(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {\"nb_lr\" : NB_LR, \"xgboost\" :xgboost, \"rnd_forest\" : randomforest}\n",
    "\n",
    "for name, m in all_models.items():\n",
    "    print name\n",
    "    setwiser = stepwise_clf(y)\n",
    "    setwiser.fit(m, train_tfidf)\n",
    "\n",
    "    X_test_lvl1 = list(setwiser.predict(test_tfidf))\n",
    "    X_train_lvl1 = list(setwiser.predict(train_tfidf))\n",
    "\n",
    "    #for sel in [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1],[1,1,1]]:\n",
    "    selections = {\"normal\" : [1,0], \"toxicOnly\" : [0,1], \"combined\" : [1,1]}\n",
    "    for sel_name, sel in selections.items():\n",
    "        sel = np.array(sel)\n",
    "        X_train_stacked = np.column_stack([X_train_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        X_test_stacked = np.column_stack([X_test_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        pd.DataFrame(np.column_stack([train[\"id\"].values, X_train_stacked])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/train_1703_FG_%s_%s.csv\" % (name, sel_name), header= False, index= False)\n",
    "        pd.DataFrame(np.column_stack([test[\"id\"].values, X_test_stacked])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/test_1703_FG_%s_%s.csv\" % (name, sel_name), header= False, index= False)   \n",
    "        print(X_train_stacked.shape)\n",
    "        print(X_test_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {\"nn\" : nn_1vsall_clf}\n",
    "\n",
    "for name, m in all_models.items():\n",
    "    print name\n",
    "    setwiser = stepwise_clf(y)\n",
    "    setwiser.fit(m, train_tfidf)\n",
    "\n",
    "    X_test_lvl1 = list(setwiser.predict(test_tfidf))\n",
    "    X_train_lvl1 = list(setwiser.predict(train_tfidf))\n",
    "\n",
    "    #for sel in [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1],[1,1,1]]:\n",
    "    selections = {\"normal\" : [1,0], \"toxicOnly\" : [0,1], \"combined\" : [1,1]}\n",
    "    for sel_name, sel in selections.items():\n",
    "        sel = np.array(sel)\n",
    "        X_train_stacked = np.column_stack([X_train_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        X_test_stacked = np.column_stack([X_test_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        pd.DataFrame(np.column_stack([train[\"id\"].values, X_train_stacked])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/train_1703_FG_%s_%s.csv\" % (name, sel_name), header= False, index= False)\n",
    "        pd.DataFrame(np.column_stack([test[\"id\"].values, X_test_stacked])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/test_1703_FG_%s_%s.csv\" % (name, sel_name), header= False, index= False)   \n",
    "        print(X_train_stacked.shape)\n",
    "        print(X_test_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98821 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312735/312735 [00:01<00:00, 183754.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "import gensim.models.word2vec as w2v\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Generate common embedings\n",
    "tokenizer = text.Tokenizer(num_words=len(val_words))\n",
    "tokenizer.fit_on_texts(list(list_sentences_f_train) + list(list_sentences_f_test))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "maxlen = 150\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_f_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_f_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "list_senteces_all = list_sentences_f_train + list_sentences_f_test\n",
    "for i in tqdm(range(len(list_senteces_all))):\n",
    "    list_senteces_all[i] = list_senteces_all[i].split(\" \")\n",
    "    \n",
    "# hyper parameters of the word2vec model\n",
    "num_features = 300 # dimensions of each word embedding\n",
    "min_word_count = 1 # this is not advisable but since we need to extract\n",
    "# feature vector for each word we need to do this\n",
    "context_size = 7 # context window length\n",
    "downsampling = 1e-3 # downsampling for very frequent words\n",
    "seed = 1 # seed for random number generator to make results reproducible\n",
    "\n",
    "word2vec_ = w2v.Word2Vec(\n",
    "    sg = 1, seed = seed,\n",
    "    workers = 4,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context_size,\n",
    "    sample = downsampling\n",
    ")\n",
    "\n",
    "word2vec_.build_vocab(list_senteces_all)\n",
    "word2vec_.train(list_senteces_all, total_examples = word2vec_.corpus_count, epochs = 10)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i ==0: print word\n",
    "    if word in word2vec_.wv:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = word2vec_.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "max_features = 98822\n",
    "embed_size = 300\n",
    "maxlen = 150\n",
    "\n",
    "\n",
    "class lstm:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "    \n",
    "    def get_mdl(self, X, y):\n",
    "        max_features = 98822\n",
    "        embed_size = 300\n",
    "        maxlen = 150\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "        x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(50, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X, y, batch_size=512, epochs=20)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            for i in range(y.shape[1]):\n",
    "                m = self.get_mdl(X, y[:,i])\n",
    "                self.models.append((m))\n",
    "        else:\n",
    "            m = self.get_mdl(X, y)\n",
    "            self.models.append((m))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], len(self.models)))\n",
    "        for i in range(len(self.models)):\n",
    "            m = self.models[i]\n",
    "            preds[:,i] = m.predict(X).flatten()\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm\n",
      "fitting all\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 191s 1ms/step - loss: 0.1521 - acc: 0.9445\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0867 - acc: 0.9668\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0688 - acc: 0.9734\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0531 - acc: 0.9799\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0393 - acc: 0.9857\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0294 - acc: 0.9897\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0224 - acc: 0.9924\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0177 - acc: 0.9940\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0145 - acc: 0.9950\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0123 - acc: 0.9958\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0102 - acc: 0.9966\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0084 - acc: 0.9972\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0074 - acc: 0.9974\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0066 - acc: 0.9978\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0055 - acc: 0.9980\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0049 - acc: 0.9984\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0047 - acc: 0.9983\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0041 - acc: 0.9984\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0038 - acc: 0.9985\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0035 - acc: 0.9986\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 191s 1ms/step - loss: 0.0471 - acc: 0.9896\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0215 - acc: 0.9906\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0174 - acc: 0.9920\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0139 - acc: 0.9937\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0103 - acc: 0.9953\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0080 - acc: 0.9966\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0064 - acc: 0.9972\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0054 - acc: 0.9976\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0045 - acc: 0.9981\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0042 - acc: 0.9980\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0037 - acc: 0.9984\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0035 - acc: 0.9986\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0028 - acc: 0.9987\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0024 - acc: 0.9990\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0022 - acc: 0.9991\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0022 - acc: 0.9990\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0022 - acc: 0.9990\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 191s 1ms/step - loss: 0.0972 - acc: 0.9670\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0420 - acc: 0.9832\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0334 - acc: 0.9868\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0251 - acc: 0.9906\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0179 - acc: 0.9936\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0129 - acc: 0.9955\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0096 - acc: 0.9966\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0076 - acc: 0.9975\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0058 - acc: 0.9980\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0049 - acc: 0.9982\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0044 - acc: 0.9985\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0038 - acc: 0.9986\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0035 - acc: 0.9986\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0031 - acc: 0.9988\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0031 - acc: 0.9988\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0025 - acc: 0.9990\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0020 - acc: 0.9992\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0021 - acc: 0.9991\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 192s 1ms/step - loss: 0.0322 - acc: 0.9968\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0117 - acc: 0.9970\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0062 - acc: 0.9978\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0035 - acc: 0.9988\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0016 - acc: 0.9994\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 8.1606e-04 - acc: 0.9997\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 7.3136e-04 - acc: 0.9998\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 7.1588e-04 - acc: 0.9998\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 5.4417e-04 - acc: 0.9998\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 5.3107e-04 - acc: 0.9998\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 5.7983e-04 - acc: 0.9998\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 4.0695e-04 - acc: 0.9999\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 4.7114e-04 - acc: 0.9998\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 4.1007e-04 - acc: 0.9999\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 3.8101e-04 - acc: 0.9999\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 189s 1ms/step - loss: 4.7360e-04 - acc: 0.9998\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 3.7045e-04 - acc: 0.9999\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 192s 1ms/step - loss: 0.1142 - acc: 0.9615\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0592 - acc: 0.9753\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0486 - acc: 0.9798\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0384 - acc: 0.9845\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0293 - acc: 0.9888\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0223 - acc: 0.9917\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0179 - acc: 0.9933\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0148 - acc: 0.9945\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0119 - acc: 0.9957\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0102 - acc: 0.9961\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0089 - acc: 0.9968\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0078 - acc: 0.9971\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0069 - acc: 0.9974\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0061 - acc: 0.9975\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0058 - acc: 0.9978\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0050 - acc: 0.9981\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0049 - acc: 0.9981\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0042 - acc: 0.9983\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0038 - acc: 0.9984\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0037 - acc: 0.9986\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 192s 1ms/step - loss: 0.0566 - acc: 0.9871\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0202 - acc: 0.9926\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0138 - acc: 0.9944\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 190s 1ms/step - loss: 0.0092 - acc: 0.9967\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0060 - acc: 0.9978\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0045 - acc: 0.9985\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0034 - acc: 0.9989\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0027 - acc: 0.9991\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0025 - acc: 0.9992\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0021 - acc: 0.9993\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0020 - acc: 0.9993\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 9.5572e-04 - acc: 0.9997\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 9.8748e-04 - acc: 0.9996\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 9.2980e-04 - acc: 0.9996\n",
      "fitting toxic or not\n",
      "Epoch 1/20\n",
      "159571/159571 [==============================] - 192s 1ms/step - loss: 0.1619 - acc: 0.9387\n",
      "Epoch 2/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0858 - acc: 0.9678\n",
      "Epoch 3/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0682 - acc: 0.9740\n",
      "Epoch 4/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0525 - acc: 0.9805\n",
      "Epoch 5/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0383 - acc: 0.9863\n",
      "Epoch 6/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0285 - acc: 0.9901\n",
      "Epoch 7/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0214 - acc: 0.9928\n",
      "Epoch 8/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0168 - acc: 0.9943\n",
      "Epoch 9/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 10/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0115 - acc: 0.9962\n",
      "Epoch 11/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0094 - acc: 0.9969\n",
      "Epoch 12/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0081 - acc: 0.9973\n",
      "Epoch 13/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 14/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0062 - acc: 0.9978\n",
      "Epoch 15/20\n",
      "159571/159571 [==============================] - 188s 1ms/step - loss: 0.0053 - acc: 0.9982\n",
      "Epoch 16/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 17/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0042 - acc: 0.9986\n",
      "Epoch 18/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0036 - acc: 0.9987\n",
      "Epoch 19/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0030 - acc: 0.9989\n",
      "Epoch 20/20\n",
      "159571/159571 [==============================] - 189s 1ms/step - loss: 0.0036 - acc: 0.9987\n",
      "(159571, 1)\n",
      "(153164, 1)\n",
      "(159571, 7)\n",
      "(153164, 7)\n",
      "(159571, 6)\n",
      "(153164, 6)\n"
     ]
    }
   ],
   "source": [
    "all_models = {\"lstm\" : lstm}\n",
    "\n",
    "for name, m in all_models.items():\n",
    "    print name\n",
    "    setwiser = stepwise_clf(y)\n",
    "    setwiser.fit(m, X_t)\n",
    "\n",
    "    X_test_lvl1 = list(setwiser.predict(X_te))\n",
    "    X_train_lvl1 = list(setwiser.predict(X_t))\n",
    "\n",
    "    #for sel in [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1],[1,1,1]]:\n",
    "    selections = {\"normal\" : [1,0], \"toxicOnly\" : [0,1], \"combined\" : [1,1]}\n",
    "    for sel_name, sel in selections.items():\n",
    "        sel = np.array(sel)\n",
    "        X_train_stacked = np.column_stack([X_train_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        X_test_stacked = np.column_stack([X_test_lvl1[i]  for i in np.where(sel==1)[0]])\n",
    "        pd.DataFrame(np.column_stack([train[\"id\"].values, X_train_stacked])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/train_1703_FG_%s_%s.csv\" % (name, sel_name), header= False, index= False)\n",
    "        pd.DataFrame(np.column_stack([test[\"id\"].values, X_test_stacked])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/test_1703_FG_%s_%s.csv\" % (name, sel_name), header= False, index= False)   \n",
    "        print(X_train_stacked.shape)\n",
    "        print(X_test_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
