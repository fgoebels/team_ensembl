{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from  keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import copy\n",
    "\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train_cleaned_no-stopwords.csv\")\n",
    "test = pd.read_csv(\"./test_cleaned_no-stopwords.csv\")\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312735\n",
      "203883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5443/159571 [00:00<00:02, 54370.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:03<00:00, 47566.03it/s]\n",
      "100%|██████████| 153164/153164 [00:02<00:00, 56325.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_words(corpus):\n",
    "    word_counts = {}\n",
    "    for sentence in corpus:\n",
    "        for word in set(sentence.split()):\n",
    "                if word not in word_counts: word_counts[word] = 0\n",
    "                word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "def remove_low_freq_words(text, val_w):\n",
    "    text = set(text.split())\n",
    "    text = list(text & val_w)\n",
    "    if len(text) == 0:\n",
    "        text = [\"CVxTz\"]\n",
    "    text = \" \".join(text)    \n",
    "    return text\n",
    "        \n",
    "all_text = list(list_sentences_train) + list(list_sentences_test)\n",
    "print len(all_text)\n",
    "word_counts = count_words(all_text)\n",
    "print len(word_counts)\n",
    "\n",
    "val_words = set()\n",
    "for k, v in word_counts.items():\n",
    "    if v > 1:\n",
    "        val_words.add(k)\n",
    "print(len(val_words))\n",
    "list_sentences_f_train  = [\"CVxTz\"] * len(list_sentences_train)\n",
    "list_sentences_f_test   = [\"CVxTz\"] * len(list_sentences_test)\n",
    "\n",
    "for i in tqdm(range(len(list_sentences_train))):\n",
    "    list_sentences_f_train[i] = remove_low_freq_words(list_sentences_train[i], val_words)\n",
    "\n",
    "for i in tqdm(range(len(list_sentences_test))):\n",
    "    list_sentences_f_test[i] = remove_low_freq_words(list_sentences_test[i], val_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list_sentences_f_train + list_sentences_f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retired vandalisms since please edits voted template new reverted dolls username explanation gas fan york fac hardcore metallica closure made remove page talk'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "def splitter(x): return x.split(\" \")\n",
    "\n",
    "all_sentences_bagofwords = vec.fit_transform(all_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_lda = LDA()\n",
    "this_lda.fit(all_sentences_bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = vec.transform(list_sentences_f_train)\n",
    "test_bow = vec.transform(list_sentences_f_test)\n",
    "\n",
    "train_lda = this_lda.transform(train_bow)\n",
    "test_lda = this_lda.transform(test_bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(np.column_stack([train[\"id\"].values, train_lda])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/train_1703_FG_LDA.csv\", header= False, index= False)\n",
    "pd.DataFrame(np.column_stack([test[\"id\"].values, test_lda])).to_csv(\"/home/ubuntu/kaggle/toxicity/team_ensembl/Florian/out/test_1703_FG_LDA.csv\", header= False, index= False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00416695, 0.0458333 , 0.05962139, 0.00416712, 0.55320212,\n",
       "       0.0041667 , 0.00416667, 0.00416746, 0.00416752, 0.31634078])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lda[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4199744 , 0.02000002, 0.02000143, 0.02000212, 0.42      ,\n",
       "       0.02      , 0.02000301, 0.0200013 , 0.02000495, 0.02001277])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lda[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
